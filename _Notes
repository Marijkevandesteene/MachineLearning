
@Dries: fout bij SVcla voor damage occured.
Door standardizing is de al dan niet damage niet meer binair, maar 


# Vragen::



Te bekijken 
lift curve

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

Creating a lift curve directly out of the box in Python for a Random Forest Classifier may not be as straightforward as some other evaluation metrics like ROC curves or precision-recall curves. However, you can still create a lift curve by following these steps:

    Make predictions using your Random Forest Classifier on a test dataset.
    Sort the predictions in descending order of probability.
    Divide the predictions into bins or segments.
    Calculate the cumulative sum of the target variable (e.g., number of positive cases) in each bin.
    Plot the lift curve using libraries like Matplotlib or Seaborn.

While Python libraries like scikit-learn provide tools for common evaluation metrics, creating a lift curve may require a bit more custom implementation. If you're interested, you can explore custom functions or packages that may assist in generating lift curves for your Random Forest Classifier.

If you'd like more guidance on this or any other related topic, feel free to ask for further assistance!



For us, the lift curve is an interesting tool, as it shows how much benefit the model has when you use it for selection. Typically, the cutoff is also made by assessing the two quantities shown on the lift curve: either one cuts the list off at a certain amount of benefit ("this will make my list that much purer"), or one cuts it off at a desired sample ("I'm able to contact 300 clients, which coincides with 10% of the population"). Note that the first logic is slightly suspect: usually there is a discrepancy between how you got the data, and how your future actions will work ("my data are from an administrative data base, and now I will use an email to contact new clients"), which means that the exact numbers are to be taken with a grain of salt. The second logic is better: if you have to contact 300 clients, these are probably the 300 best you can find.

# Plotting lift curve on the logreg4 model - binary classification
import scikitplot as skplt
skplt.metrics.plot_lift_curve(y_true=y_test, y_probas=y_pred_prob4, figsize=(8, 6));


# Learning curve
What is more training data worth to me?
should I invest in getting more data?
if my training is computationally too heavy or too slow, can I subsample?
is a train-test-holdout split benecial?
do the hyperparameters trained on the 80% CV data hold for the 100% model?

-----
## Outliers

* resultaten isolationForest : 9% outliers geÃ¯dentificeerd, dat lijkt me heel hoog.
Bevestigt dat er geen outliers zijn???
-----

========================================================================================


# Done::

## to do beter staven ...
Het blijkt dat in de top 20 missing features, er voor een heel aantal variabelen 53 instances missing waren. Vermoedelijk gaat het over dezelfde instances ging.
Na export op basis van 1 van de opgelijste features en bleek dat dit inderdaad voor deze instances het geval was.

Enkel voor deze waarden is er info:
- spa_ic is 21x = 1 (2/21:  outcome_damage_ic = 1)
- empl_ic is 40x = 0 (12/40:  outcome_damage_ic = 1)
- married_cd is 53x = false (12/53::  outcome_damage_ic = 1)
- claims_am is 20x = 0 (6/20:  outcome_damage_ic = 1)

- spa_ic & claims_am & empl_ic is 6x ingevuld

## tenure_mts / tenunre


Some conclusions (Bino)
- no missing values inthe outcomes
- a lot of missings in the scores
- tenure_mts does have almost 10% missing --> perhaps we should not just drop it, but re-use the years !!
<=> 

- It appears that for missing tenure_mts, tenure_ys is also missing
- Check wether either tenure_yrs or tenure_mts is missing returns an empty DF
(do this before updating missing values)
print("shape of dataframe where either tenure_mts of tenure_yrs is missing",train_V2[train_V2.loc[:,['tenure_mts','tenure_yrs']].isnull().sum(axis=1) == 1].shape)
'shape of dataframe where either tenure_mts of tenure_yrs is missing (0, 53)'

## scores uitmiddelen en categorische waarde gebruiken om aan te geven welke waarden al dan niet gebruikt werden
het blijkt dat dat tot meer dan 30 combinaties leidt


## kNN imputer for missing values


## isolationForest to evaluate anomalies


## RF_model for profit: RandomforestRegressor (without outliers)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 150, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.95, 'max_depth': 80, 'bootstrap': True}
Train R2: 0.942
Test R2: 0.748


# RF_model for oucome_damage: RandomforestRegressor
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 150, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 0.7, 'max_depth': 5, 'bootstrap': True}
Train R2: 0.231
Test R2: 0.079

## RF_model for outcome_damage_inc: RandomforestClassifier (2m15s)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimal hyperparameter values according to our random search: 
{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 5, 'max_depth': None, 'bootstrap': False}
Train accuracy of the refitted model: 1.000
Test accuracy of the refitted model: 0.739
              precision    recall  f1-score   support

   no damage       0.75      0.97      0.85      1100
      damage       0.48      0.08      0.14       385

    accuracy                           0.74      1485
   macro avg       0.62      0.53      0.49      1485
weighted avg       0.68      0.74      0.66      1485


## RF_model for profit: RandomforestRegressor
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 210, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.98, 'max_depth': None, 'bootstrap': True}
Train R2: 0.942
Test R2: 0.808

## RF_model for oucome_damage: RandomforestRegressor
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 240, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 0.98, 'max_depth': 5, 'bootstrap': True}
Train R2: 0.237
Test R2: 0.040

## RF_model for outcome_damage_inc: RandomforestClassifier (2m15s)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimal hyperparameter values according to our random search: 
{'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.8, 'max_depth': 27, 'bootstrap': True}
Train accuracy of the refitted model: 0.888
Test accuracy of the refitted model: 0.738
              precision    recall  f1-score   support

   no damage       0.76      0.95      0.84      1459
      damage       0.50      0.14      0.22       519

    accuracy                           0.74      1978
   macro avg       0.63      0.54      0.53      1978
weighted avg       0.69      0.74      0.68      1978


## RF_model for profit: Halving (21s)
itting 5 folds for each of 2 candidates, totalling 10 fits
Optimal hyperparameter values according to our random search: 
{'n_estimators': 210, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 0.7, 'max_depth': 16, 'bootstrap': True}
Train accuracy of the refitted model: 0.886
Test accuracy of the refitted model: 0.743
              precision    recall  f1-score   support

   no damage       0.76      0.96      0.85      1459
      damage       0.54      0.14      0.23       519

    accuracy                           0.74      1978
   macro avg       0.65      0.55      0.54      1978
weighted avg       0.70      0.74      0.68      1978

## RF_model fo
## Visual evaluation of CV results against accuracy / R2
for parameters: 'param_max_depth', 'param_min_samples_leaf', 'param_n_estimators'
- 

## Extra check for tenure_mts = tenure_yrs
It appears that for missing tenure_mts, tenure_ys is also missing
=> Either tenure_yrs of tenure_mts is missing returns an empty DF
train_V2[train_V2.loc[:,['tenure_yrs','tenure_mts']].isnull().sum(axis=1) == 1].shape
=> 0


## Added learning curve and python file with common_used_functions
+ Learning curve

## Looked at applying data 'manipulation' to score

## Explainability

Looking at tree-based feature importances we see that according to 
1. feature importance 
	0	profit_am	0.346637
	1	nights_booked	0.323773
	2	income_am	0.041613
	3	profit_last_am	0.040630
	4	presidential	0.036040
2. Permutation feature importance
	0	nights_booked	1.371450	0.139278
	1	profit_am	1.355326	0.057785
	2	presidential	0.058581	0.002814
	3	income_am	0.034528	0.000321
	4	shop_am	0.032894	0.001874
3. drop-column feature importance
	0	nights_booked	0.084504
	1	profit_am	0.044256
	2	presidential	0.012767
	3	cab_requests	0.002097
	4	tenure_mts	0.001627
	
	
Looking at Linear surrogate explainability models we see that according to 
1. Univariate regression: 
	Most positive univariate betas:
		presidential      2.792305
		gold_status       0.965458
		lactose_ic        0.619158
		client_segment    0.321293
		gluten_ic         0.308610

	Most negative univariate betas:
		avg_score_pos   -0.231379
		gender          -0.197266
		insurance_ic    -0.136384
		dining_ic       -0.036779
		divorce         -0.031825
		
This techniques indicates other features as more important than tree-based feature importance. 

2. Multiple linear regression models
             feature      coef
	0         profit_am  0.389688
	1     nights_booked -0.315662
	2      presidential  0.185609
	3    profit_last_am  0.112224
	4         income_am  0.102620

The features with the highest coefficients(absolute) are the features that were the most important features from the tree based techniques


# Executive summary (prompt in chatgpt)

Write an executive summary to convince a hotel manager to apply the selected list of hotel guests. The list of hotel guests
was determined running several algorithms on the training data. Cross-validation was applied to tune hyperparamters and following algorithms were used Random-forest, gbm, sVR.

The best model was selected. 

The gain (152189.787) calculated using the revenue of selection (495762.401) from the best performing model and was compared to the revenue of a random sample (343572.614)




# 

=======================================================================================================================





+  The data seems to be evenly distributed, no shuffling is needed

* add_indicator voegt per feature met missing values (8) een variabele toe om aan te geven of de feature == imputed




Trainen Random Forest

- standardizing is not needed 


