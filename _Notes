

# Vragen::

Te bekijken 
lift curve


For us, the lift curve is an interesting tool, as it shows how much benefit the model has when you use it for selection. Typically, the cutoff is also made by assessing the two quantities shown on the lift curve: either one cuts the list off at a certain amount of benefit ("this will make my list that much purer"), or one cuts it off at a desired sample ("I'm able to contact 300 clients, which coincides with 10% of the population"). Note that the first logic is slightly suspect: usually there is a discrepancy between how you got the data, and how your future actions will work ("my data are from an administrative data base, and now I will use an email to contact new clients"), which means that the exact numbers are to be taken with a grain of salt. The second logic is better: if you have to contact 300 clients, these are probably the 300 best you can find.

# Plotting lift curve on the logreg4 model - binary classification
import scikitplot as skplt
skplt.metrics.plot_lift_curve(y_true=y_test, y_probas=y_pred_prob4, figsize=(8, 6));




-----
## Outliers

* resultaten isolationForest : 9% outliers geÃ¯dentificeerd, dat lijkt me heel hoog.
Bevestigt dat er geen outliers zijn???
-----

========================================================================================


# Done::

## to do beter staven ...
Het blijkt dat in de top 20 missing features, er voor een heel aantal variabelen 53 instances missing waren. Vermoedelijk gaat het over dezelfde instances ging.
Na export op basis van 1 van de opgelijste features en bleek dat dit inderdaad voor deze instances het geval was.

Enkel voor deze waarden is er info:
- spa_ic is 21x = 1 (2/21:  outcome_damage_ic = 1)
- empl_ic is 40x = 0 (12/40:  outcome_damage_ic = 1)
- married_cd is 53x = false (12/53::  outcome_damage_ic = 1)
- claims_am is 20x = 0 (6/20:  outcome_damage_ic = 1)

- spa_ic & claims_am & empl_ic is 6x ingevuld

## tenure_mts / tenunre


Some conclusions (Bino)
- no missing values inthe outcomes
- a lot of missings in the scores
- tenure_mts does have almost 10% missing --> perhaps we should not just drop it, but re-use the years !!
<=> 

- It appears that for missing tenure_mts, tenure_ys is also missing
- Check wether either tenure_yrs or tenure_mts is missing returns an empty DF
(do this before updating missing values)
print("shape of dataframe where either tenure_mts of tenure_yrs is missing",train_V2[train_V2.loc[:,['tenure_mts','tenure_yrs']].isnull().sum(axis=1) == 1].shape)
'shape of dataframe where either tenure_mts of tenure_yrs is missing (0, 53)'

## scores uitmiddelen en categorische waarde gebruiken om aan te geven welke waarden al dan niet gebruikt werden
het blijkt dat dat tot meer dan 30 combinaties leidt


## kNN imputer for missing values


## isolationForest to evaluate anomalies


## RF_model for revenue: RandomforestRegressor
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 247, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 104, 'max_depth': 7, 'bootstrap': True}
Train R2: 0.990
Test R2: 0.955

Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 121, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 58, 'max_depth': 18, 'bootstrap': True}
Train R2: 0.990
Test R2: 0.956


## RF_model for outcome_damage_inc: RandomforestClassifier
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimal hyperparameter values according to our random search: 
{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 119, 'max_depth': 2, 'bootstrap': False}
Train accuracy of the refitted model: 1.000
Test accuracy of the refitted model: 1.000
              precision    recall  f1-score   support

   no damage       1.00      1.00      1.00       724
      damage       1.00      1.00      1.00       266

    accuracy                           1.00       990
   macro avg       1.00      1.00      1.00       990
weighted avg       1.00      1.00      1.00       990

## RF_model for oucome_damage: RandomforestRegressor
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Optimally found RF hyperparams after this random search: 
{'n_estimators': 163, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 111, 'max_depth': 28, 'bootstrap': True}
Train R2: 1.000
Test R2: 1.000


## Visual evaluation of CV results against accuracy / R2
for parameters: 'param_max_depth', 'param_min_samples_leaf', 'param_n_estimators'
- 

## Extra check for tenure_mts = tenure_yrs
It appears that for missing tenure_mts, tenure_ys is also missing
=> Either tenure_yrs of tenure_mts is missing returns an empty DF
train_V2[train_V2.loc[:,['tenure_yrs','tenure_mts']].isnull().sum(axis=1) == 1].shape
=> 0


## Added learning curve and python file with common_used_functions
+ Learning curve

## Looked at applying data 'manipulation' to score

## Explainability

Looking at tree-based feature importances we see that according to 
1. feature importance 
	0	profit_am	0.346637
	1	nights_booked	0.323773
	2	income_am	0.041613
	3	profit_last_am	0.040630
	4	presidential	0.036040
2. Permutation feature importance
	0	nights_booked	1.371450	0.139278
	1	profit_am	1.355326	0.057785
	2	presidential	0.058581	0.002814
	3	income_am	0.034528	0.000321
	4	shop_am	0.032894	0.001874
3. drop-column feature importance
	0	nights_booked	0.084504
	1	profit_am	0.044256
	2	presidential	0.012767
	3	cab_requests	0.002097
	4	tenure_mts	0.001627
	
	
Looking at Linear surrogate explainability models we see that according to 
1. Univariate regression: 
	Most positive univariate betas:
		presidential      2.792305
		gold_status       0.965458
		lactose_ic        0.619158
		client_segment    0.321293
		gluten_ic         0.308610

	Most negative univariate betas:
		avg_score_pos   -0.231379
		gender          -0.197266
		insurance_ic    -0.136384
		dining_ic       -0.036779
		divorce         -0.031825
		
This techniques indicates other features as more important than tree-based feature importance. 

2. Multiple linear regression models
             feature      coef
	0         profit_am  0.389688
	1     nights_booked -0.315662
	2      presidential  0.185609
	3    profit_last_am  0.112224
	4         income_am  0.102620

The features with the highest coefficients(absolute) are the features that were the most important features from the tree based techniques


# Executive summary (prompt in chatgpt)

Write an executive summary to convince a hotel manager to apply the selected list of hotel guests. The list of hotel guests
was determined running several algorithms on the training data. Cross-validation was applied and following algorithms were used Random-forest, gbm, sVR




# 

=======================================================================================================================





+  The data seems to be evenly distributed, no shuffling is needed

* add_indicator voegt per feature met missing values (8) een variabele toe om aan te geven of de feature == imputed




Trainen Random Forest

- standardizing is not needed 


