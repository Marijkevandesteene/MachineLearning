{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Machine Learning with Python\n",
    "\n",
    "Submitted by : \n",
    "\n",
    "Dries Luts (dries-luts@hotmail.com)<br />\n",
    "Bino Maiheu (binomaiheu@gmail.com)<br />\n",
    "Marijke Van De Steene (marijkevandesteene@hotmail.com)<br />\n",
    "\n",
    "This notebook is submitted by the group above for the course exame \"Machine Learning with Python\", taught by Bart Van Rompaye. Course IPVW-\n",
    "ICES 2024, **due date**: 2024-07-03 23:59. \n",
    "\n",
    "# Changelog\n",
    "\n",
    "- **2024-06-05** [MV] : Initial version\n",
    "- **2024-06-06** [BM] : Consolidated structure, imported initial analysis from notebooks \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Importing packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import scipy.stats as stats\n",
    "import scikitplot as skplt \n",
    "import pickle\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler  # for preprocessing & scaling\n",
    "from sklearn.preprocessing import PolynomialFeatures  # for polynomial features preprocessing\n",
    "from sklearn.impute import SimpleImputer, KNNImputer   # for missing values imputation\n",
    "from sklearn.model_selection import train_test_split  # train-test splits\n",
    "from sklearn.model_selection import StratifiedKFold  # K-fold resampling, stratified\n",
    "from sklearn.model_selection import GridSearchCV  # Hyperparameter tuning\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Hyperparameter tuning with calibration\n",
    "from sklearn.calibration import calibration_curve  # calibration curve plotting\n",
    "from sklearn.calibration import CalibrationDisplay  # calibration curve plotting\n",
    "from sklearn.metrics import confusion_matrix  # performance metrics, confusion matrix\n",
    "from sklearn.metrics import classification_report  # performance matrix classifiaction report\n",
    "from sklearn.metrics import roc_auc_score  # Area Under Receiver Operating Characteristics\n",
    "from sklearn.metrics import roc_curve  # ROC\n",
    "from sklearn.metrics import RocCurveDisplay  # ROC plotting\n",
    "from sklearn.metrics import accuracy_score  # performance metric accuracy (0/1) score\n",
    "from sklearn.metrics import precision_score  # performance metric\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression modelling\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # GBM for classification\n",
    "from sklearn.svm import SVC  # SVM for classification\n",
    "from sklearn.utils import resample  # Resampling\n",
    "from imblearn.over_sampling import SMOTE  # Synthetic upsampling\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Setting plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Jupyter magic command to show plots inline immediately\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Setting seed\n",
    "seed = 43\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Loading the house price dataset\n",
    "data_file_path = Path('input')  # Set to the path of folder where you can find 'train_V2.csv' and 'score.csv'\n",
    "\n",
    "train_filename = data_file_path / 'train_V2.csv'\n",
    "score_filename = data_file_path / 'score.csv'\n",
    "dict_filename = data_file_path / 'dictionary.csv'\n",
    "\n",
    "# -- Training data\n",
    "train_V2 = pd.read_csv(train_filename)\n",
    "score = pd.read_csv(score_filename)\n",
    "dictionary = pd.read_csv(dict_filename, sep=';')\n",
    "\n",
    "# -- Some feedback \n",
    "print('Training set shape: {}' .format(train_V2.shape))\n",
    "print('Score set shape: {}' .format(score.shape))\n",
    "print('Dictionary set shape: {}' .format(dictionary.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyse some high level stuff regarding the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print list of features\n",
    "print('Training set features : ')\n",
    "print(train_V2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- compare the feaures in the training & score sets\n",
    "print(\"Features in the training set but not in the scoring set (target variables) : \")\n",
    "set(train_V2.columns).difference(set(score.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Adding an index to the \n",
    "train_V2.insert(0, 'Id', range(0, 0 + len(train_V2)))\n",
    "if 'Id' in train_V2.columns:\n",
    "    train_V2 = train_V2.set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Print some info\n",
    "train_V2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Ok, now that we have our data loaded, lets dive into the anlysis.  In this section we shall check for consistency, handle missing values, outliers etc... We first start  with extracting categorical and numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## a very important consideration DATA PREPARATION is needed for score\n",
    "\n",
    "is that everything you do has to be redone for the scoring set, exactly as it was done for the training set! To avoid a mess with mean imputations and categorization etc, I will just stitch the two together, and do the changes - this is allowed, as long as you don't use the outcome!\n",
    "\n",
    "In the example solution the score and traning data are 'merged' (without the outcome) are treated in the same way and then divided.\n",
    "Is that a good approach?\n",
    "\n",
    "### Use data from the score set for imputing missing values???\n",
    "To decide on the approach for missing values???\n",
    "\n",
    "Misschien in ieder geval ook meebekijken, als een van de features in de score lijkt te ontbreken en deze heeft een grote impact in het model, ... heeft dit dan impact op de voorspelling,\n",
    "je introduceert een fout (guassian fout) bij een voorspelling van een feature / bij imputing a feature.\n",
    "### Misschien is het beter een feature niet mee te nemen in het model als deze voor alle te scoren clienten ontbreekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of categorical and numerical features\n",
    "\n",
    "It's not clear immediately what the categorical and numerical features are in the dataset, this is important for later on (e.g. imputation of missing values), so we spend a little time analysing this. \n",
    "\n",
    "\n",
    "Here we aim to get a list of feature names (i.e. column names) one with categorical features, one with numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : insert code from Dries with analysis code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in the end we want 2 lists, one with the name of categorial features, the numerical features and the target features\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "target_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature types\n",
    "\n",
    "Some observations in the output of `train_V2.info()` above here: \n",
    "\n",
    "- all the variables seem to be numeric (encoded as float64), except for:\n",
    "- `gender` : object contains 'M' or 'V', we will replace those with 0 and 1 for consistency with the other variables\n",
    "- `married_cd` : appears to be a boolean, so clearly this is categorical\n",
    "\n",
    "Let's first look at the feature types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- re-assign the gender to 0 or 1 \n",
    "train_V2['gender'] = train_V2['gender'].map({'M': 0, 'V': 1}) # M = 0, V = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram analysis of the score variables\n",
    "\n",
    "When looking a bit more closely to the distribution of the score variables, we noticed that score5_neg did not conform to the rest of the data. In the introductory document to the exam, it was stated that these score variables represented quantiles. Likely in case of hotel 5, this is still a raw score which has not been converted to a quantile yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    train_V2[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    train_V2[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's in case of hotel5 convert this score to a quantile value. To do this, we calculate the percentile rank for the score. An alternative would be to get the quantiles from the empirical cumulative distribution function, or rescale the distribution to zero mean and unit variance, assume it's shape to be - let's say - Gaussian and compute the quantiles from that cdf. But let's keep things simple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[\"score5_neg_uniform\"]  = train_V2[\"score5_neg\"].rank(method='max', pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- a small check\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,12))\n",
    "train_V2[\"score5_neg\"].hist(ax=axs[0][0])\n",
    "train_V2[\"score5_neg_uniform\"].hist(ax=axs[0][1])\n",
    "\n",
    "axs[1][0].plot(train_V2[\"score5_neg\"], train_V2[\"score5_pos\"], '.')\n",
    "axs[1][1].plot(train_V2[\"score5_neg_uniform\"], train_V2[\"score5_pos\"], '.')\n",
    "\n",
    "axs[0][0].set_title(\"score5_neg histogram\")\n",
    "axs[0][1].set_title(\"score5_neg_uniform histogram\")\n",
    "axs[1][0].set_title(\"score5_neg vs score5_pos\")\n",
    "axs[1][1].set_title(\"score5_neg_uniform vs score5_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- and now replace the variable in the dataset\n",
    "train_V2['score5_neg'] = train_V2['score5_neg_uniform']\n",
    "train_V2.drop(columns=['score5_neg_uniform'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- make a small plot to check the resutls:\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    train_V2[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    train_V2[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still appears a bit strange that the `score5_neg`variable is now so uniform afer converstion to a percentile rank in comparison to the other scores, but let's leave it at that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of equal features\n",
    "\n",
    "It seemed strange that there is a variable called `tenure`, once expressed in months, once in years. So lets look a bit closer to the relation between `tenure_mts` and `tenure_yrs` via a scatterplot below : \n",
    "From the dictionary one could \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- make a scatterplot \n",
    "sns.scatterplot(data=train_V2, x='tenure_mts', y='tenure_yrs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[(train_V2['divorce']==0) & (train_V2['married_cd']==0)].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both express the same variable, once expressed in years, once in months. This becomes even more explicit when plotting `12*tenure_yrs`versus `tenure_mts` so it probably makes no sense to include both, let's keep `tenure_mts` and drop the `tenure_yrs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that for missing tenure_mts, tenure_ys is also missing\n",
    "# Either tenure_yrs of tenure_mts is missing returns an empty DF\n",
    "print(\"shape of dataframe where either tenure_mts of tenure_yrs is missing\",train_V2[train_V2.loc[:,['tenure_mts','tenure_yrs']].isnull().sum(axis=1) == 1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.drop(columns=['tenure_yrs'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling of  missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology and TODO's \n",
    "\n",
    "1. Getting missing values descending per feature\n",
    "2. Verwerken van de scores\n",
    "3. Find instances with missing observations (% of missing for a lot of features is equal > it appears these values for these featues are missing for the same instances)\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- [] beter staven waarom we effectief dan nog 53 wegsmijten\n",
    "- [] de scores uitmiddelen, maar een categorische variabele invoeren die aangeeft van welk hotel afkomstig --> zie: https://github.com/Marijkevandesteene/MachineLearning/issues/8\n",
    "- [] KNNImputer gebruiken, maar wel features herschalen hiervoor\n",
    "- [] evt. ook es die IterativeImputer gebruiken (is multivariaat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an idea about total missing values\n",
    "total_missings = train_V2.isnull().sum().sort_values(ascending=False)  # total missng values, sorted\n",
    "print(\"Top 20 of most missing features : \")\n",
    "total_missings.head(40)  # Show top 20 most missing features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missings.plot(kind='bar', figsize=(16,4), title=\"Number of missing values per feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an idea about percentage missing values\n",
    "pct_missings = train_V2.isnull().mean().sort_values(ascending=False)  # average (%) missng values, sorted\n",
    "pct_missings.head(53)  # Show top 20 most missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_vars = [ f\"score{i+1}_{xx}\" for i in range(5) for xx in (\"pos\", \"neg\") ]\n",
    "other_vars = list(set(train_V2.columns).difference(score_vars))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,6), gridspec_kw={'width_ratios': [1, 4]})\n",
    "\n",
    "pct_missings[score_vars].plot(kind='bar', ax=axs[0])\n",
    "pct_missings[other_vars].plot(kind='bar', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some conclusions\n",
    "- no missing values inthe outcomes\n",
    "- a lot of missings in the scores\n",
    "- tenure_mts does have almost 10% missing --> perhaps we should not just drop it, but re-use the years !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of the score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[\"score_pos\"] = train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].mean(axis=1)\n",
    "train_V2[\"score_neg\"] = train_V2[[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2['not_null_pos_columns'] = train_V2.apply(lambda row: [col for col in [\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"] if pd.notnull(row[col])], axis=1)\n",
    "train_V2['not_null_neg_columns'] = train_V2.apply(lambda row: [col for col in [\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"] if pd.notnull(row[col])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_V2.head()\n",
    "train_V2['not_null_pos_columns'].value_counts().to_csv('not_null_pos_columns.csv')\n",
    "train_V2['not_null_neg_columns'].value_counts().to_csv('not_null_neg_columns.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "train_V2[\"score_neg\"].hist(ax=axs[0])\n",
    "train_V2[\"score_pos\"].hist(ax=axs[1])\n",
    "axs[0].set_title(\"score_neg\")\n",
    "axs[1].set_title(\"score_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the features missing mor than 35% (score*_pos, neg)\n",
    "print(f\"Shape of train_V2 BEFORE dropping missing features: {train_V2.shape}\")\n",
    "missing_a_lot = pct_missings[pct_missings > 0.35].index  # we take from all variables those missing most, and take the row-idx\n",
    "print(f\"Columns missing more than 15% :{missing_a_lot}\")\n",
    "train_V2 = train_V2.drop(missing_a_lot, axis=1)\n",
    "print(f\"Shape of train_V2 AFTER dropping missing features: {train_V2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do beter staven ...\n",
    "Het blijkt dat in de top 20 missing features, er voor een heel aantal variabelen 53 instances missing waren. Vermoedelijk gaat het over dezelfde instances ging.\n",
    "Na export op basis van 1 van de opgelijste features en bleek dat dit inderdaad voor deze instances het geval was.\n",
    "\n",
    "Enkel voor deze waarden is er info:\n",
    "- spa_ic is 21x = 1 (2/21:  outcome_damage_ic = 1)\n",
    "- empl_ic is 40x = 0 (12/40:  outcome_damage_ic = 1)\n",
    "- married_cd is 53x = false (12/53::  outcome_damage_ic = 1)\n",
    "- claims_am is 20x = 0 (6/20:  outcome_damage_ic = 1)\n",
    "\n",
    "- spa_ic & claims_am & empl_ic is 6x ingevuld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a number of instances (53) data seems to be missing for a list of features. These will be eliminated from the dataset\n",
    "#missing_data.to_csv('missing_data.csv')\n",
    "instances_missingsData = train_V2[train_V2.loc[:,['company_ic','claims_no','income_am','gold_status','nights_booked','gender','shop_am','retired','fam_adult_size','children_no','divorce','profit_last_am','sport_ic','crd_lim_rec','credit_use_ic','gluten_ic','lactose_ic','insurance_ic','prev_all_in_stay','profit_am','bar_no','age','marketing_permit','urban_ic']].isnull().sum(axis=1) == 24]\n",
    "print(instances_missingsData.shape)\n",
    "print(instances_missingsData.index)\n",
    "train_V2 = train_V2.drop(instances_missingsData.index)\n",
    "\n",
    "# drop the rows, we already dropped 10 columns, so be careful here... , better way to code this up\n",
    "#drop_rows = train_V2[train_V2.isnull().sum(axis=1) > 30 ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = train_V2.isnull().sum().sort_values(ascending=False)\n",
    "print(\"These are the features for which we still have missing values : \")\n",
    "missing_rows[missing_rows>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn-imputer voor missing scores: add_indicator and rescale\n",
    "\n",
    "What is the add_indicator parameter in the imputers?\n",
    "In addition to imputing the missing values, the imputers have an add_indicator parameter that marks the values that were missing, which might carry some information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "\n",
    "Pipeline KNNImputer / IterativeImputer\n",
    "https://stackoverflow.com/questions/64900801/implementing-knn-imputation-on-categorical-variables-in-an-sklearn-pipeline#:~:text=My%20pipeline%20includes%20sklearn%27s%20KNNImputer%20estimator,impute%20categorical%20features%20in%20my%20dataset.&text=My%20pipeline%20includes%20sklearn%27s,features%20in%20my%20dataset.&text=includes%20sklearn%27s%20KNNImputer%20estimator,impute%20categorical%20features%20in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_feats = missing_rows[missing_rows>0].index.to_list()\n",
    "print(\"Feature with missing values : \")\n",
    "print(missing_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_feats_categorical = ['presidential', 'dining_ic', 'shop_use']\n",
    "missing_feats_continuous = ['tenure_mts', 'neighbor_income', 'cab_requests','score_pos','score_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the missing features, with SimpleImputer we have to select between continuous & categorical features\n",
    "# probably here as well, for now we're just using the uniform weights, but check afterwards if the categorical\n",
    "# values are ok\n",
    "imputer_knn = KNNImputer(n_neighbors=5, weights='uniform', add_indicator=False).set_output(transform=\"pandas\")\n",
    "\n",
    "train_V2 = imputer_knn.fit_transform(X=train_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total amount of missings\n",
    "total_total_missings = train_V2.isnull().sum().sum()\n",
    "print(f'Are there any missings at all anymore, if this is zero, there are none: {total_total_missings}')\n",
    "# YAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly and outlier detection\n",
    "\n",
    "\n",
    "Hoeven niet noodzakelijk dingen eruit te zwieren, maar wel minstens aangeven dat we er naar gekeken hebben en argumenteren waarom er niets uit gaat. \n",
    "\n",
    "11% wordt aangeduid als een anomaly. \n",
    "Dat lijkt het vermoeden dat er niet echt outliers zijn te bevestigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can not handle missing values\n",
    "# Fitting default isolation forest for anomaly/outlier detection\n",
    "# Importing the correct class as usual\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Checking which hyperparameters are available\n",
    "# print(f\"Hyperparamerters for IsolationForest class: {IsolationForest().get_params()}\")\n",
    "\n",
    "# Initializing model\n",
    "if_model = IsolationForest(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Fitting (only X data, because unsupervised)\n",
    "X_train_V2 = train_V2.drop(columns=['outcome_profit','outcome_damage_inc','outcome_damage_amount'], inplace=False)\n",
    "if_model.fit(X=X_train_V2)\n",
    "\n",
    "# Predicting on the same data\n",
    "y_pred_train = if_model.predict(X=X_train_V2)\n",
    "\n",
    "# Checking frequency table of predicted values\n",
    "print('Frequency table of predicted values:')\n",
    "pd.Series(y_pred_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations and exploratory analysis\n",
    "\n",
    "Now that we have reasonably clean data, let's perform some initial exploratory analysis, correlation plots, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome to maximize is profit - damage\n",
    "\n",
    "# don't assign to dataframe just yet ???\n",
    "train_V2['revenue'] = train_V2['outcome_profit'] - train_V2['outcome_damage_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking univariate distribution of the revenue\n",
    "sns.displot(train_V2['revenue']);  # With seaborn for a change\n",
    "plt.xticks(rotation=45); # Rotating x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I guess a scatter plot would've done well also\n",
    "\n",
    "plt.scatter(x = range(0, 0 + len(train_V2)),y=train_V2['revenue'], alpha=0.5);  # alpha=0.5 makes it a bit see through\n",
    "plt.xlabel('Id');\n",
    "plt.ylabel('revenue');\n",
    "plt.title('Alternative: scatter plot');\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = 'neighbor_income'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x=train_V2[x], y=train_V2['outcome_profit']);\n",
    "plt.scatter(x=train_V2[x], y=train_V2['outcome_damage_amount']);\n",
    "plt.title('profit and damage');\n",
    "plt.xlabel(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot-type for year built vs SalePrice\n",
    "plt.figure(figsize=(25, 10), dpi=500)  # Bumping up image size and DPI for better viewing\n",
    "sns.boxplot(x='divorce', y='outcome_profit', data=train_V2);\n",
    "plt.xticks(rotation=90);  # To rotate x-axis labels\n",
    "plt.title('Relationship of divorce versus profit');\n",
    "plt.show()  # remember: necessary to do this when trying to plot multiple plots from a single cell!\n",
    "\n",
    "# But I guess a scatter plot would've done well also\n",
    "plt.scatter(x=train_V2['income_am'], y=train_V2['outcome_profit'], alpha=0.5);  # alpha=0.5 makes it a bit see through\n",
    "plt.xlabel('income_am');\n",
    "plt.ylabel('outcome_profit');\n",
    "plt.title('Alternative: scatter plot of income versus profit');\n",
    "plt.show() \n",
    "\n",
    "# Correlation matrix between features\n",
    "#corrmat = train_V2.corr(numeric_only=True)  # Since Pandas 2.0 you need to supply this attribute\n",
    "corrmat = train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit'], axis=1).corr(numeric_only=True) \n",
    "plt.figure(figsize=(12, 12));\n",
    "sns.heatmap(corrmat, vmax=1, square=True);\n",
    "# Note, we keep SalePrice in here as well, proceed with caution (no data snooping!)\n",
    "\n",
    "# Scatterplot matrix (might take a while)\n",
    "plot_cols = ['outcome_damage_inc', 'income_am', 'profit_last_am', 'profit_am', 'damage_am', 'damage_inc', 'crd_lim_rec']\n",
    "sns.pairplot(train_V2[plot_cols], height=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing : NOT NEEDED FOR RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().set_output(transform='pandas')\n",
    "train_v2_stan_df = scaler.fit_transform(X=train_V2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "\n",
    "Goal of the exam : What do we need to predict? Is it available as outcome in our data?\n",
    "1. the revenue per client (= profit - damage)\n",
    "    - needs to be calculated\n",
    "2. predict which clients will cause damage\n",
    "    - outcome_damage_inc\n",
    "3. predict the amount of damage fot those who will cause damage / wreak havoc\n",
    "    - outcome_damage_amount\n",
    "\n",
    "Calculate revenue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- es trainen op apart amount profit & damage, maar ook es op verschil (revenue)\n",
    "\n",
    "## verdeling\n",
    "\n",
    "- Bino : GBM\n",
    "- Marijke : RF\n",
    "- Dries : SVR/SVC\n",
    "- Logistic regression for question 2 + Model performance evaluation: lift curve / Calibration Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "### Model to predict revenue per client\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "1. the revenue per client (= profit - damage)\n",
    "    - needs to be calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split (Note: we're taking 30% test set size here instead of 20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit'], axis=1), # features DF\n",
    "                                                    train_V2['revenue'],   # target DF/series\n",
    "                                                    test_size=0.3, # 30% as test or validation set (who cares about the exact names)\n",
    "                                                    shuffle=True,  # This shuffles the data! (Important)\n",
    "                                                    random_state=seed)  # setting seed for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for classification - random search 3-fold CV - this can take a while\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=20)]  # list comprehension because we want integers!\n",
    "max_features = [int(x) for x in np.linspace(start=5, stop=150, num=20)] \n",
    "max_depth = [int(x) for x in np.linspace(start=2, stop=50, num=10)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "print('Optimally found RF hyperparams after this random search: \\n{}' .format(rf_hyperparam_tuning_random.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_random_best = rf_hyperparam_tuning_random.best_estimator_\n",
    "rf_random_best.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_random_best_y_train_pred = rf_random_best.predict(X=X_train)\n",
    "rf_random_best_y_test_pred = rf_random_best.predict(X=X_test)\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train R2: %.3f' % rf_random_best.score(X=X_train, y=y_train))\n",
    "print('Test R2: %.3f' % rf_random_best.score(X=X_test, y=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_random_best, open('RF_model_revenue.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning_random.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_leaf', 'param_n_estimators']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV R2')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture brings some nuance to the 'best' random forests: let's start with `max_depth`: apparently, you can find similarly good trees at lower depth; but the very lowest depths perform rather bad. This may give us a bit of benefit in terms of generalization error. \n",
    "\n",
    "For the minimal samples per leaf (`min_samples_leaf`), the picture really prefers not to shy away from small leafs: it seems that going into unique values is worth it. Finally, let's see whether we need a lot of trees or not.\n",
    "\n",
    "With regards to the amount of trees `n_estimators`: you don't seem to necessarily need many trees in your forest to get a good model. This is good to know for future training, but realize that with more complex forests, you may need more trees to improve generalization error. So let's not restrict the forests too much in the grid search. \n",
    "\n",
    "Now, do realize that these kinds of analyses are to be taken with a grain of salt in the section on GBMs we go 1 small step beyond and make plot that take 2 hyperparameters at once in consideration, which might already help a bit more. But for now, we'll keep it simple.\n",
    "\n",
    "Another word of caution: sometimes, due to randomness, the best was at the boundary of the search space, let's increase the options a bit more in the grid search. Since we don't care too much about the size of the object, we're probably better of grid searching higher values, but with less options (this keeps the training time low)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict which clients will cause damage\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "\n",
    "2. predict which clients will cause damage\n",
    "    - outcome_damage_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test set split for output = output_damage_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split (Note: we're taking 30% test set size here instead of 20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','revenue','outcome_profit'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_damage_inc'],   # target DF/series\n",
    "                                                    test_size=0.2, # 30% as test or validation set (who cares about the exact names)\n",
    "                                                    shuffle=True,  # This shuffles the data! (Important)\n",
    "                                                    random_state=seed)  # setting seed for consistent results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED for RF\n",
    "# I'll also make standardized (normalized) versions\n",
    "scaler = StandardScaler().set_output(transform='pandas')\n",
    "scaler.fit(X=X_train)\n",
    "X_train_norm = scaler.transform(X=X_train)\n",
    "X_test_norm = scaler.transform(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for classification - random search 3-fold CV - this can take a while\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=5)]  # list comprehension because we want integers!\n",
    "max_features = [int(x) for x in np.linspace(start=5, stop=150, num=20)] \n",
    "max_depth = [int(x) for x in np.linspace(start=2, stop=40, num=10)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning = RandomizedSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Executing / fitting the random search \n",
    "rf_hyperparam_tuning.fit(X=X_train, y=y_train)\n",
    "print('Optimal hyperparameter values according to our random search: \\n{}' .format(rf_hyperparam_tuning.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_opt1 = rf_hyperparam_tuning.best_estimator_\n",
    "rf_opt1.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_opt1_y_train_pred = rf_opt1.predict(X=X_train)\n",
    "rf_opt1_y_test_pred = rf_opt1.predict(X=X_test)\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train accuracy of the refitted model: %.3f' % rf_opt1.score(X=X_train, y=y_train))\n",
    "print('Test accuracy of the refitted model: %.3f' % rf_opt1.score(X=X_test, y=y_test))\n",
    "\n",
    "# Classification report \n",
    "target_names = ['no damage', 'damage']\n",
    "print(classification_report(y_true=y_test, y_pred=rf_opt1_y_test_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_opt1, open('RF_model_damage_inc.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_leaf', 'param_n_estimators']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV R2')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict the amount of damage per client\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "\n",
    "3. predict the amount of damage fot those who will cause damage / wreak havoc\n",
    "    - outcome_damage_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-stest set split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split (Note: we're taking 30% test set size here instead of 20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['revenue','outcome_damage_inc','outcome_profit'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_damage_amount'],   # target DF/series\n",
    "                                                    test_size=0.3, # 30% as test or validation set (who cares about the exact names)\n",
    "                                                    shuffle=True,  # This shuffles the data! (Important)\n",
    "                                                    random_state=seed)  # setting seed for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for classification - random search 3-fold CV - this can take a while\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=20)]  # list comprehension because we want integers!\n",
    "max_features = [int(x) for x in np.linspace(start=5, stop=150, num=20)] \n",
    "max_depth = [int(x) for x in np.linspace(start=2, stop=50, num=10)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "print('Optimally found RF hyperparams after this random search: \\n{}' .format(rf_hyperparam_tuning_random.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_random_best = rf_hyperparam_tuning_random.best_estimator_\n",
    "rf_random_best.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_random_best_y_train_pred = rf_random_best.predict(X=X_train)\n",
    "rf_random_best_y_test_pred = rf_random_best.predict(X=X_test)\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train R2: %.3f' % rf_random_best.score(X=X_train, y=y_train))\n",
    "print('Test R2: %.3f' % rf_random_best.score(X=X_test, y=y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_random_best, open('RF_model_damage_amount.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning_random.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_leaf', 'param_n_estimators']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV R2')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do Predict probability / Calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do: list of 200 / selection of hotel guest with the highest revenue\n",
    "# For EXECUTIVE SUMMARY : Compare the list of selected hotel guests across different algorithm and select the top 200 identified by all three approaches???\n",
    "\n",
    "1. Gradient Booster\n",
    "2. Random Forest\n",
    "3. SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
