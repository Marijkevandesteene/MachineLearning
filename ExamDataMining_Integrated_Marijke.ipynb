{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Machine Learning with Python\n",
    "\n",
    "Submitted by : \n",
    "\n",
    "Dries Luts (dries-luts@hotmail.com)<br />\n",
    "Bino Maiheu (binomaiheu@gmail.com)<br />\n",
    "Marijke Van De Steene (marijkevandesteene@hotmail.com)<br />\n",
    "\n",
    "This notebook is submitted by the group above for the course exame \"Machine Learning with Python\", taught by Bart Van Rompaye. Course IPVW-\n",
    "ICES 2024, **due date**: 2024-07-03 23:59. \n",
    "\n",
    "# Changelog\n",
    "\n",
    "- **2024-06-05** [MV] : Initial version\n",
    "- **2024-06-06** [BM] : Consolidated structure, imported initial analysis from notebooks \n",
    "- **2024-06-18** [BM] : Consolidated structure: Walkthrough in team / Finalized data preparation\n",
    "- **2024-06-18** [MVDS] : Added data preparation steps on score\n",
    "- **2024-06-19** [BM] : Fixed issue w.r.t kNN imputer to apply for score\n",
    "- **2024-06-23** [MVDS] : Random forest model tuning, Calibration / applied explainability / removed some try out code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Importing packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import scipy.stats as stats\n",
    "import scikitplot as skplt \n",
    "import pickle\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import Machine learning libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # for preprocessing & scaling\n",
    "from sklearn.preprocessing import PolynomialFeatures  # for polynomial features preprocessing\n",
    "from sklearn.impute import SimpleImputer, KNNImputer   # for missing values imputation\n",
    "from sklearn.model_selection import train_test_split  # train-test splits\n",
    "from sklearn.model_selection import StratifiedKFold  # K-fold resampling, stratified\n",
    "from sklearn.model_selection import GridSearchCV  # Hyperparameter tuning\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Hyperparameter tuning with calibration\n",
    "from sklearn.calibration import calibration_curve  # calibration curve plotting\n",
    "from sklearn.calibration import CalibrationDisplay  # calibration curve plotting\n",
    "from sklearn.metrics import confusion_matrix  # performance metrics, confusion matrix\n",
    "from sklearn.metrics import classification_report  # performance matrix classifiaction report\n",
    "from sklearn.metrics import roc_auc_score  # Area Under Receiver Operating Characteristics\n",
    "from sklearn.metrics import roc_curve  # ROC\n",
    "from sklearn.metrics import RocCurveDisplay  # ROC plotting\n",
    "from sklearn.metrics import accuracy_score  # performance metric accuracy (0/1) score\n",
    "from sklearn.metrics import precision_score  # performance metric\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression modelling\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # GBM for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor  # KNN\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest for classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM for classification\n",
    "from sklearn.svm import SVC  # SVM for classification\n",
    "from sklearn.utils import resample  # Resampling\n",
    "from imblearn.over_sampling import SMOTE  # Synthetic upsampling\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import Defined functions\n",
    "import common_used_functions as cuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Setting plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Jupyter magic command to show plots inline immediately\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Setting seed\n",
    "seed = 43\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Loading the dataset for scoring Russian hotel guests\n",
    "data_file_path = Path('input')  # Set to the path of folder where you can find 'train_V2.csv' and 'score.csv'\n",
    "\n",
    "train_filename = data_file_path / 'train_V2.csv'\n",
    "score_filename = data_file_path / 'score.csv'\n",
    "dict_filename = data_file_path / 'dictionary.csv'\n",
    "\n",
    "# -- Training & scoring data\n",
    "train_V2 = pd.read_csv(train_filename)\n",
    "score = pd.read_csv(score_filename)\n",
    "\n",
    "# -- We load the dictionare as a dict\n",
    "#    Note that small edits were made to the original file \n",
    "#    - tenure_yrs occurred twice, this was corrected\n",
    "#    - income_am did not occur, was added...\n",
    "dictionary = pd.read_csv(dict_filename, sep=';', header=None).set_index(0).to_dict(orient=\"dict\")[1]\n",
    "\n",
    "# -- Some feedback \n",
    "print('Training set shape: {}' .format(train_V2.shape))\n",
    "print('Score set shape: {}' .format(score.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyse some high level stuff regarding the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print list of features\n",
    "print('Training set features : ')\n",
    "print(train_V2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- compare the feaures in the training & score sets\n",
    "print(\"Features in the training set but not in the scoring set (target variables) : \")\n",
    "set(train_V2.columns).difference(set(score.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Adding an index to the \n",
    "train_V2.insert(0, 'Id', range(0, 0 + len(train_V2)))\n",
    "if 'Id' in train_V2.columns:\n",
    "    train_V2 = train_V2.set_index('Id')\n",
    "\n",
    "# 0-500 voor score of lengte train_v2 > lengte  + 500?\n",
    "\n",
    "score.insert(0, 'Id', range(0, 0 + len(score)))\n",
    "if 'Id' in score.columns:\n",
    "    score = score.set_index('Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Print some info\n",
    "train_V2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Ok, now that we have our data loaded, lets dive into the anlysis.  In this section we shall check for consistency, handle missing values, outliers etc... We first start  with extracting categorical and numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of categorical and numerical features\n",
    "\n",
    "It's not clear immediately what the categorical and numerical features are in the dataset, this is important for later on (e.g. imputation of missing values), so we spend a little time analysing this. \n",
    "\n",
    "\n",
    "Here we aim to get a list of feature names (i.e. column names) one with categorical features, one with numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- test the number of unique values in the dataframe\n",
    "train_nunique = train_V2.nunique()\n",
    "train_nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nunique[ train_nunique == 2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see quite some variables with only 2 unique values, and from looking at the dictionary, those likely will be binary features : true/false. \n",
    "\n",
    "Additionally, we see that the features `client_segment` as well as `sect_empl` also only contain 6 unique values and from the dictionary (client and employment sectors), it is safe to assume that those features are also categorical. Some additional features also contain a limited amount of unique values, however, judging from the dictionary those should not be considered categorical : \n",
    "- `fam_adult_size`: number of adults in family\n",
    "- `children_no` : number of children\n",
    "\n",
    "So, in summary we can create the following classification for the features (categorical/numerical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the categorical input features are those with 2 unique values, as well as client_segment and sect_empl, but not the ones which start with outcome, those are target features\n",
    "categorical_input_features = [ 'client_segment', 'sect_empl', *train_nunique[ (train_nunique == 2) & ~train_nunique.index.str.startswith('outcome_') ].index.to_list() ]\n",
    "\n",
    "# the numerical input features the other ones but not the ones which start with outcome, those are target features\n",
    "numerical_input_features = train_V2.columns[ ~train_V2.columns.isin(categorical_input_features) & ~train_V2.columns.str.startswith('outcome_')].to_list()\n",
    "\n",
    "# these are the target features in the dataframe\n",
    "target_features = ['outcome_profit', 'outcome_damage_amount', 'outcome_damage_inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical input features\n",
    "\n",
    "These are the resulting categorical input features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in categorical_input_features:\n",
    "    print(f\"- {ft} : {dictionary[ft]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical input features\n",
    "These are the resulting numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in numerical_input_features:\n",
    "    print(f\"- {ft} : {dictionary[ft]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target features\n",
    "These are the target features (initially... in the further analysis, this will be refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in target_features:\n",
    "    print(f\"- {ft} : {dictionary[ft]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on memory consumption\n",
    "\n",
    "Ideally, to reduce the **memory footprint** of this data, we reallocate the categorical variables to a smaller integer, or even a boolean. Currently the variables are in memory as 64 bit floats which is way to large. At the moment we leave things as they are & come back to this if time permits. Note that we will make some alterations onwards as some features, like gender are \"objects\", i.e. strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature types\n",
    "\n",
    "Some observations in the output of `train_V2.info()` above here: \n",
    "\n",
    "- all the variables seem to be numeric (encoded as float64), except for:\n",
    "- `gender` : object contains 'M' or 'V', we will replace those with 0 and 1 for consistency with the other variables\n",
    "- `married_cd` : appears to be a boolean, so clearly this is categorical\n",
    "\n",
    "Let's first look at the feature types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- re-assign the gender to 0 or 1 \n",
    "train_V2['gender'] = train_V2['gender'].map({'M': 0, 'V': 1}) # M = 0, V = 1\n",
    "score['gender'] = score['gender'].map({'M': 0, 'V': 1}) # M = 0, V = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram analysis of the score variables\n",
    "\n",
    "When looking a bit more closely to the distribution of the score variables, we noticed that score5_neg did not conform to the rest of the data. In the introductory document to the exam, it was stated that these score variables represented quantiles. Likely in case of hotel 5, this is still a raw score which has not been converted to a quantile yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    train_V2[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    train_V2[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram score\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    score[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    score[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's in case of hotel5 convert this score to a quantile value. To do this, we have to be a bit carefull. An easy way to handle this would be to calculate the percentile rank for the score, e.g. : \n",
    "\n",
    "```python\n",
    "train_V2[\"score5_neg_uniform\"]  = train_V2[\"score5_neg\"].rank(method='max', pct=True)\n",
    "```\n",
    "\n",
    "However, it would be rather difficult to apply this consistently on the independent score dataset later on. So, as an alternative we will get the quantiles from the cumulative distribution function, we can do this via the emprical cdf or rescale the distribution to zero mean and unit variance, assuming it's shape to be - let's say - Gaussian and compute the quantiles from that cdf, keeping things simple, we opted to calculate the z-score, assume the distirbution is roughly Gaussian(without rigorously checking, as this probably won't affect the result significantly) in the following way, using `scipy.stats.norm.cdf`. For discussion see: https://github.com/Marijkevandesteene/MachineLearning/issues/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[\"score5_neg_uniform\"] = ( train_V2[\"score5_neg\"] - train_V2[\"score5_neg\"].mean() ) / train_V2[\"score5_neg\"].std()\n",
    "train_V2[\"score5_neg_uniform\"] = stats.norm.cdf(train_V2[\"score5_neg_uniform\"])\n",
    "score[\"score5_neg_uniform\"] = ( score[\"score5_neg\"] - train_V2[\"score5_neg\"].mean() ) / train_V2[\"score5_neg\"].std()\n",
    "score[\"score5_neg_uniform\"] = stats.norm.cdf(score[\"score5_neg_uniform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- a small check\n",
    "fig, axs = plt.subplots(2,2, figsize=(8,8))\n",
    "train_V2[\"score5_neg\"].hist(ax=axs[0][0])\n",
    "train_V2[\"score5_neg_uniform\"].hist(ax=axs[0][1])\n",
    "\n",
    "axs[1][0].plot(train_V2[\"score5_neg\"], train_V2[\"score5_pos\"], '.')\n",
    "axs[1][1].plot(train_V2[\"score5_neg_uniform\"], train_V2[\"score5_pos\"], '.')\n",
    "\n",
    "axs[0][0].set_title(\"score5_neg histogram\")\n",
    "axs[0][1].set_title(\"score5_neg_uniform histogram\")\n",
    "axs[1][0].set_title(\"score5_neg vs score5_pos\")\n",
    "axs[1][1].set_title(\"score5_neg_uniform vs score5_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- a similar check on score data\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,12))\n",
    "score[\"score5_neg\"].hist(ax=axs[0][0])\n",
    "score[\"score5_neg_uniform\"].hist(ax=axs[0][1])\n",
    "\n",
    "axs[1][0].plot(train_V2[\"score5_neg\"], train_V2[\"score5_pos\"], '.')\n",
    "axs[1][1].plot(train_V2[\"score5_neg_uniform\"], train_V2[\"score5_pos\"], '.')\n",
    "\n",
    "axs[0][0].set_title(\"score5_neg histogram\")\n",
    "axs[0][1].set_title(\"score5_neg_uniform histogram\")\n",
    "axs[1][0].set_title(\"score5_neg vs score5_pos\")\n",
    "axs[1][1].set_title(\"score5_neg_uniform vs score5_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- and now replace the variable in the dataset for training data and for score\n",
    "train_V2['score5_neg'] = train_V2['score5_neg_uniform']\n",
    "train_V2.drop(columns=['score5_neg_uniform'], inplace=True)\n",
    "score['score5_neg'] = score['score5_neg_uniform']\n",
    "score.drop(columns=['score5_neg_uniform'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- make a small plot to check the results:\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    train_V2[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    train_V2[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20,6))\n",
    "for k in range(5):     \n",
    "    score[f\"score{k+1}_pos\"].hist(ax=axs[0][k])\n",
    "    score[f\"score{k+1}_neg\"].hist(ax=axs[1][k])\n",
    "\n",
    "    axs[0][k].set_title(f\"score{k+1}_pos\")\n",
    "    axs[1][k].set_title(f\"score{k+1}_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency between damage incurred and damage amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max damage amount when no damage is incurred : {train_V2.loc[train_V2['outcome_damage_inc'] == 0, 'outcome_damage_amount'].max()}\")\n",
    "print(f\"Min damage amount when damage is incurred : {train_V2.loc[train_V2['outcome_damage_inc'] == 1, 'outcome_damage_amount'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha, so there are cases for which there is damage incurred, but the amount of the damage is 0 ... interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling of  missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology and TODO's \n",
    "\n",
    "1. Getting missing values descending per feature\n",
    "2. Verwerken van de scores\n",
    "3. Find instances with missing observations (% of missing for a lot of features is equal > it appears these values for these featues are missing for the same instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an idea about total missing values\n",
    "total_missings = train_V2.isnull().sum().sort_values(ascending=False)  # total missng values, sorted\n",
    "print(\"Top 20 of most missing features : \")\n",
    "total_missings.head(20)  # Show top 20 most missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missings.plot(kind='bar', figsize=(16,4), title=\"Number of missing values per feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an idea about percentage missing values\n",
    "pct_missings = train_V2.isnull().mean().sort_values(ascending=False)  # average (%) missng values, sorted\n",
    "#pct_missings.head(20)  # Show top 20 most missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_vars = [ f\"score{i+1}_{xx}\" for i in range(5) for xx in (\"pos\", \"neg\") ]\n",
    "other_vars = list(set(train_V2.columns).difference(score_vars))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,6), gridspec_kw={'width_ratios': [1, 4]})\n",
    "\n",
    "axs[0].set_ylabel('pct. missing [%]')\n",
    "axs[1].set_ylabel('pct. missing [%]')\n",
    "\n",
    "pct_missings[score_vars].multiply(100.).plot(kind='bar', ax=axs[0])\n",
    "pct_missings[other_vars].multiply(100.).plot(kind='bar', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some conclusions :\n",
    "- no missing values inthe outcomes\n",
    "- a lot of missing values in the scores given by the hotels (> 70% of the data), however despite the large quantity of missing data, it doesn't seem a good idea to throw away this data as the score given by other hotels in the chain may be a very relevant predictor, so we decide to keep these features and treat the missing values (which in its own right may be relevant information as well). \n",
    "- tenure_mts does have almost 10% missing --> perhaps we should not just drop it, but re-use the years !!\n",
    "- it's striking that a large number of features report the same amount of missing values, so these are probably a set of rows which we can drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment of the score values\n",
    "\n",
    "The imputation of the score values is somewhat tricky. There is potentially a lot of information encoded in there, but also > 70 % of missing values. First of all we do not know from the dataset what hotels correspond to 1, 2, ... 5 also we do not know from the description that hotel1 from the training set corresponds to hotel1 in the score set. By aggregating the scores (.e.g. calculating a mean over the hotels for both the positivity and negativity scores, we can already get rid of a lot of missing values, however the downside is that the information on what hotel did give the score is lost then). Still this may be a good way forward as we don't know for sure whether the hotels in the training & score set correspond (though it's probably safe to assume so). One caveat with this approach is that we would be averaging quantile values, which may not be entirely consistent so it would probably be better to average the original scores, but then we would have to make assumptions on the underlying distribution of these original scores. For a discussion we refer to : https://github.com/Marijkevandesteene/MachineLearning/issues/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[\"avg_score_pos\"] = train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].mean(axis=1)\n",
    "train_V2[\"avg_score_neg\"] = train_V2[[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"]].mean(axis=1)\n",
    "\n",
    "score[\"avg_score_pos\"] = train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].mean(axis=1)\n",
    "score[\"avg_score_neg\"] = train_V2[[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "train_V2[\"avg_score_neg\"].hist(ax=axs[0])\n",
    "train_V2[\"avg_score_pos\"].hist(ax=axs[1])\n",
    "axs[0].set_title(\"avg_score_neg\")\n",
    "axs[1].set_title(\"avg_score_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "score[\"avg_score_neg\"].hist(ax=axs[0])\n",
    "score[\"avg_score_pos\"].hist(ax=axs[1])\n",
    "axs[0].set_title(\"avg_score_neg\")\n",
    "axs[1].set_title(\"avg_score_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing percentages when computing the mean\n",
    "train_V2[[\"avg_score_pos\", \"avg_score_neg\"]].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that is we compute the mean, we already reduce the number of missings to about 20 - 30 %, which is already an improvement w.r.t. the ~70 % missing values on average per hotel. The fact however that scores for hotel guests are missing may also be a relevant features, so we decide to include that as an additional feature, but in fact we can also count how many hotels have given the score, so 0 will be no hotels (i.e. missing) and then 1 -> 5 for how many hotels have given the score. That way we include the maximum amount of information from the original scores, only neglecting the actual hotels which have given the score which we don't know anyway which is which from the data given and whether or not the hotel labels are consistent between training & score set. That way we can think of imputing an average score to keep those records in the dataset, as we know it was originally missing (i.e. num_score_pos/neg == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- assign the missings to a categorical variable\n",
    "# train_V2[\"score_pos_missing\"] = train_V2[\"avg_score_pos\"].isna()\n",
    "# train_V2[\"score_neg_missing\"] = train_V2[\"avg_score_neg\"].isna()\n",
    "\n",
    "# -- count the number of hotels which have given a score, so 0 means it was originally missing for all the hotels\n",
    "train_V2[\"num_score_pos\"] = (~train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].isnull()).sum(axis=1)\n",
    "train_V2[\"num_score_neg\"] = (~train_V2[[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"]].isnull()).sum(axis=1)\n",
    "\n",
    "score[\"num_score_pos\"] = (~train_V2[[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"]].isnull()).sum(axis=1)\n",
    "score[\"num_score_neg\"] = (~train_V2[[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"]].isnull()).sum(axis=1)\n",
    "\n",
    "# -- and add it to the categorical variables\n",
    "#categorical_input_features.extend([\"score_pos_missing\", \"score_neg_missing\"])\n",
    "numerical_input_features = [ ft for ft in numerical_input_features if not ft.startswith(\"score\") ]\n",
    "numerical_input_features.extend([\"avg_score_pos\", \"avg_score_neg\", \"num_score_pos\", \"num_score_neg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- now drop the original scores from the dataset and from the score dataset\n",
    "train_V2.drop(columns=[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"], inplace=True)\n",
    "train_V2.drop(columns=[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"], inplace=True)\n",
    "\n",
    "score.drop(columns=[\"score1_pos\", \"score2_pos\", \"score3_pos\", \"score4_pos\", \"score5_pos\"], inplace=True)\n",
    "score.drop(columns=[\"score1_neg\", \"score2_neg\", \"score3_neg\", \"score4_neg\", \"score5_neg\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment of the tenure_yrs variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis above we also noticed a significant amount of missing values for `tenure_yrs` and `tenure_mts`. What is also striking is that probably both variables express the same quantity, once expressed in months and once in years. Let's visualise that in a scatter plot : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[[\"tenure_mts\", \"tenure_yrs\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- make a scatterplot \n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "axs[0].plot(train_V2['tenure_mts'], 12.*train_V2['tenure_yrs'], '.')\n",
    "axs[0].set_xlabel(\"tenure_mts\")\n",
    "axs[0].set_ylabel(\"12 * tenure_yrs\")\n",
    "train_V2[\"tenure_mts\"].hist(ax=axs[1])\n",
    "axs[1].set_xlabel(\"tenure_mts\")\n",
    "(12.*train_V2[\"tenure_yrs\"]).hist(ax=axs[2])\n",
    "axs[2].set_xlabel(\"12 * tenure_yrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check if both variables contain the same missings as imputing missing `tenure_mts` from `tenure_yrs` would be a straighforward choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- get the missing values for both\n",
    "tenure_num_values = train_V2[[\"tenure_mts\", \"tenure_yrs\"]].isna().sum(axis=1)\n",
    "# -- test how many only have 1 missing value, so either tenure_mts or tenure_yrs, but not both present or absent\n",
    "tenure_num_values[tenure_num_values == 1].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- get the missing values for both in the score dataset\n",
    "tenure_num_values = score[[\"tenure_mts\", \"tenure_yrs\"]].isna().sum(axis=1)\n",
    "# -- test how many only have 1 missing value, so either tenure_mts or tenure_yrs, but not both present or absent\n",
    "tenure_num_values[tenure_num_values == 1].any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, so `tenure_mts` `tenure_yrs` either both occurr in the dataset, or neither of them. So clearly, both expressing the same variable, once expressed in years, once in months and having established that we can't directly use either one to impute missing values in the other feature, it probably makes no sense to include both. Let's therefore omit the one with the lowest granularity : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.drop(columns=['tenure_yrs'], inplace=True)\n",
    "score.drop(columns=['tenure_yrs'], inplace=True)\n",
    "\n",
    "numerical_input_features.remove(\"tenure_yrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment of missing records (instances with missing a lot of information)\n",
    "\n",
    "We still see that for a large amount of features, there are somewhat 53 missings. Likely, these are records which are not complete. We can clearly see this in the histogram below, which plots a histogram of the the number of missing features per record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "print(f\"Number of features train_V2 dataset after transformations so far: {len(train_V2.columns)}\")\n",
    "print(f\"Number of features score dataset after transformations so far: {len(score.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "# -- a histogram of the amount of missing features per record\n",
    "train_V2.isnull().sum(axis=1).hist(ax=axs[0], bins=46, log=True)\n",
    "axs[0].set_xlabel(\"Missing features per record for the training dataset\")\n",
    "\n",
    "# -- a histogram of the amount of missing features per record in the score dataset \n",
    "score.isnull().sum(axis=1).hist(ax=axs[1], bins=46, log=True)\n",
    "axs[1].set_xlabel(\"Missing features per record for the score dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that for the majority of the records, the number of missing features is less than 10, however for a small amount, we have more than 35 of the features missing. It appears there are 53 such records. It makes sense to delete these rows alltogether. This is a minimal loss of data on a total of 5000 records (1.06 %). \n",
    "\n",
    "We see a similar image in the score data. There we have 4 instances. \n",
    "\n",
    "We can either place a cut on the number of missing features per record, like so : \n",
    "\n",
    "```python\n",
    "drop_records = train_V2[train_V2.isnull().sum(axis=1) > 35]\n",
    "print(f\"Number of records with > 35 missing features : {drop_records.shape[0]}\")\n",
    "train_V2.drop(index=drop_records.index, inplace=True)\n",
    "```\n",
    "\n",
    "or explicitly look at what features are missing, it turns out that : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a number of instances (53) data seems to be missing for a lot of features (24). These instances may be eliminated from the dataset\n",
    "instances_missingsData = train_V2[train_V2.loc[:,['company_ic','claims_no','income_am','gold_status','nights_booked','gender','shop_am','retired','fam_adult_size','children_no','divorce','profit_last_am','sport_ic','crd_lim_rec','credit_use_ic','gluten_ic','lactose_ic','insurance_ic','prev_all_in_stay','profit_am','bar_no','age','marketing_permit','urban_ic']].isnull().sum(axis=1) == 24]\n",
    "print(instances_missingsData.shape)\n",
    "train_V2 = train_V2.drop(instances_missingsData.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a number of instances (4) in score.csv data seems to be missing for a list of features. These will be eliminated from the score set, since comparable instances were removed from the training set\n",
    "# These are not withheld in the list of clients \n",
    "instances_missingsData = score[score.loc[:,['company_ic','claims_no','income_am','gold_status','nights_booked','gender','shop_am','retired','fam_adult_size','children_no','divorce','profit_last_am','sport_ic','crd_lim_rec','credit_use_ic','gluten_ic','lactose_ic','insurance_ic','prev_all_in_stay','profit_am','bar_no','age','marketing_permit','urban_ic']].isnull().sum(axis=1) == 24]\n",
    "print(instances_missingsData.shape)\n",
    "score = score.drop(instances_missingsData.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After export of these instances with missing more that 35 features missing, we see that there is data for the following features (other features are NaN):\n",
    "- `spa_ic` is 21x = 1 (2/21: outcome_damage_ic = 1)\n",
    "- `empl_ic` is 40x = 0 (12/40: outcome_damage_ic = 1)\n",
    "- `married_cd` is 53x = false (12/53: outcome_damage_ic = 1)\n",
    "- `claims_am` is 20x = 0 (6/20: outcome_damage_ic = 1)\n",
    "- `spa_ic` & `claims_am` & `empl_ic` has data for 6 of these instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- remaning records\n",
    "# train_V2.shape\n",
    "# number of features\n",
    "print(f\"Number of features train_V2 dataset after transformations so far: {len(train_V2.columns)}\")\n",
    "print(f\"Number of features score dataset after transformations so far: {len(score.columns)}\")\n",
    "\n",
    "# number of features\n",
    "print(f\"Shape train_V2 dataset after transformations so far: {train_V2.shape}\")\n",
    "print(f\"Shape score dataset after transformations so far: {score.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing data\n",
    "\n",
    "In this section we impute the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = train_V2.isnull().sum().sort_values(ascending=False)\n",
    "print(\"These are the features for which we still have missing values : \")\n",
    "missing_rows[missing_rows>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_feats_categorical = ['presidential', 'dining_ic', 'shop_use']\n",
    "missing_feats_continuous = ['tenure_mts', 'neighbor_income', 'cab_requests', 'avg_score_pos', 'avg_score_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features = train_V2.isnull().sum(axis=1).sort_values(ascending=False)  # total missng values, sorted\n",
    "print(\"These are the rows which contain missing features : \")\n",
    "row_idx_missing = missing_features[missing_features>0]\n",
    "print(row_idx_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of separating between numerical and categorical values, we will use a **KNNImputer** to make optimal use of possible correlations between the features. **However**, as the KNN technique is sensitive to the scale of the features (it uses a distance based metric), we first have to rescale the features before being able to use a KNNImputer. Most of our features are categorical between 0 and 1, so we'll just use a MinMaxScaler between 0 and 1 to rescale to that fixed range..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- first define the min max scaler and apply to the original data\n",
    "imputer_scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "# -- set the target features aside as we will apply the imputer to the score as well, which lacks the target features,\n",
    "#    so only use the input feature for transformation & imputing\n",
    "train_V2_outcome = train_V2[target_features]\n",
    "train_V2_scaled = imputer_scaler.fit_transform(train_V2.drop(columns=target_features))\n",
    "score_scaled = imputer_scaler.transform(score)\n",
    "\n",
    "# -- next define the imputer having 5 neighbours (default) and uniform weights\n",
    "imputer_knn = KNNImputer(n_neighbors=5, weights='uniform').set_output(transform=\"pandas\")\n",
    "\n",
    "# -- apply to the scaled data\n",
    "train_V2_scaled = imputer_knn.fit_transform(X=train_V2_scaled)\n",
    "score_scaled = imputer_knn.transform(score)\n",
    "\n",
    "# -- and apply the inverse transform\n",
    "train_V2_inv_transformed = imputer_scaler.inverse_transform(train_V2_scaled)\n",
    "score_inv_transformed = imputer_scaler.inverse_transform(score_scaled) \n",
    "\n",
    "# -- interestingly, the set_output(transform=\"pandas\") is not implemented yet on the inverse transform in sklearn,\n",
    "#    so we will pour the numpy array into a dataframe ourselves (see:  https://github.com/scikit-learn/scikit-learn/issues/27843) \n",
    "#    here we put back the outcome variables which we set aside before we fitted the scaler transform\n",
    "train_V2 = pd.concat( [ pd.DataFrame(train_V2_inv_transformed, columns=train_V2_scaled.columns).set_index(train_V2_scaled.index), train_V2_outcome ], axis=1 )\n",
    "score = pd.DataFrame(score_inv_transformed, columns=score_scaled.columns).set_index(score_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the unique values in the replaces values for the categorial features in order to \n",
    "# test whether they are categorical & no interpolated somehow\n",
    "for feat_name in missing_feats_categorical:\n",
    "    print(f\"Unique values in {feat_name} replaced missing\")\n",
    "    print(train_V2.loc[row_idx_missing.index, feat_name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ha! so we have a few little bastard slipping through, let's correct !\n",
    "train_V2.loc[train_V2['dining_ic'] < 0.5,'dining_ic'] = 0.\n",
    "train_V2.loc[train_V2['shop_use'] < 0.5,'shop_use'] = 0.\n",
    "\n",
    "score.loc[score['dining_ic'] < 0.5,'dining_ic'] = 0.\n",
    "score.loc[score['shop_use'] < 0.5,'shop_use'] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally test whether we still have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = train_V2.isnull().sum().sort_values(ascending=False)\n",
    "print(\"These are the features for which we still have missing values : \")\n",
    "missing_rows[missing_rows>0]\n",
    "\n",
    "\n",
    "print(f\"Total amount of missing values in the dataframe : {train_V2.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = score.isnull().sum().sort_values(ascending=False)\n",
    "print(\"These are the features for which we still have missing values : \")\n",
    "missing_rows[missing_rows>0]\n",
    "\n",
    "\n",
    "print(f\"Total amount of missing values in the dataframe : {score.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly and outlier detection\n",
    "\n",
    "Now, before moving onwards, we still want to have a look to possible outliers in the data in order to decide what to do with them. Let's start with some simple visualisations like boxplots and histograms to get a feeling for the distribution of the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- for the plots\n",
    "nrows = 6\n",
    "ncols = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- we first make some boxplots for every feature\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20,16))\n",
    "for idx, feat_name in enumerate(train_V2.columns):\n",
    "    row = idx // ncols\n",
    "    col = idx % ncols\n",
    "    train_V2.boxplot([feat_name], ax=axs[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- and here we do the same with the histograms\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20,16))\n",
    "for idx, feat_name in enumerate(train_V2.columns):\n",
    "    row = idx // ncols\n",
    "    col = idx % ncols\n",
    "    train_V2.hist(feat_name, bins=50, ax=axs[row][col], log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already make a couple of interesting observations : \n",
    "- There (visually) seem so be some outliers present in the distributions for `income_am`, `profit_last_am`, `damage_am`, `bar_no`, `claims_am`, and `outcome_profit`\n",
    "- There seem to be some records which show a total number of nights booked (`nights_booked`) at the chain of 0 ! This is rather surprising given that these\n",
    "\n",
    "Let's look at a few of the outliers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[train_V2['outcome_profit'] > 25000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.drop(1979, inplace=True)\n",
    "train_V2.drop(3763, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[train_V2['income_am'] > 150000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[train_V2['damage_am'] > 6000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[train_V2['bar_no'] > 100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2[train_V2['claims_am'] > 30000 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there seems to be a fairly rich person in the dataset with very high income (>200000), but also high `profit_last_am` which seems to be in client segment 5  (a higher segment seems correlated with income (not shown)), so which probably makes sense. We don't think this should necessarily be considered as an outlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis\n",
    "\n",
    "In addition to the univariate analysis above, we also looked at an unsupervised technique for outlier detection, namely **isolation forests**\n",
    "\n",
    "The output of an Isolation Forest model typically includes the anomaly score for each data point. The anomaly score is a measure of how different or isolated a data point is compared to the rest of the data.\n",
    "\n",
    "According to the isolationForest more than 10% of the data is an outlier. We believe that this percentage is to high looking at the data.\n",
    "Therefore we believe that this confirms the conclusion from the univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can not handle missing values\n",
    "# Fitting default isolation forest for anomaly/outlier detection\n",
    "# Importing the correct class as usual\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Checking which hyperparameters are available\n",
    "# print(f\"Hyperparamerters for IsolationForest class: {IsolationForest().get_params()}\")\n",
    "\n",
    "# Initializing model\n",
    "if_model = IsolationForest(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Fitting (only X data, because unsupervised)\n",
    "X_train_V2 = train_V2.drop(columns=['outcome_profit','outcome_damage_inc','outcome_damage_amount'], inplace=False)\n",
    "X_alldata = pd.concat([X_train_V2,score])\n",
    "\n",
    "if_model.fit(X=X_train_V2)\n",
    "\n",
    "# Predicting on the same data\n",
    "y_pred_train = if_model.predict(X=X_train_V2)\n",
    "\n",
    "# Checking frequency table of predicted values\n",
    "print('Frequency table of predicted values:')\n",
    "pd.Series(y_pred_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations and exploratory analysis\n",
    "\n",
    "Now that we have reasonably clean data, let's perform some initial exploratory analysis, correlation plots, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance in data?\n",
    "\n",
    "While a 25% split between classes may not be considered highly imbalanced, it could still lead to potential challenges, especially if the classes are not well-represented. It's important to assess the impact of this class distribution on your specific task and consider techniques like oversampling, undersampling, or using different evaluation metrics to address any imbalance issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(train_V2[train_V2['outcome_damage_inc'] == 1])/len(train_V2) * 100\n",
    "print('The percentage of hotel guests causing damage is {:.2f}'.format(p))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- this plots the correlation matrix\n",
    "#    we adjusted the scale and colors a little to make the positive or negative correlations stand out, using the seismic colormap for this\n",
    "corrmat = train_V2.corr(numeric_only=True)\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corrmat, vmin=-1, vmax=1, center=0, cmap=\"seismic\",  linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking univariate distribution of the revenue\n",
    "sns.displot(train_V2['outcome_profit'] - train_V2['outcome_damage_amount']);  # With seaborn for a change\n",
    "plt.xticks(rotation=45); # Rotating x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I guess a scatter plot would've done well also\n",
    "\n",
    "plt.scatter(x = range(0, 0 + len(train_V2)),y=train_V2['outcome_profit'] - train_V2['outcome_damage_amount'], alpha=0.5);  # alpha=0.5 makes it a bit see through\n",
    "plt.xlabel('Id');\n",
    "plt.ylabel('revenue');\n",
    "plt.title('Alternative: scatter plot');\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = 'neighbor_income'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x=train_V2[x], y=train_V2['outcome_profit']);\n",
    "plt.scatter(x=train_V2[x], y=train_V2['outcome_damage_amount']);\n",
    "plt.title('profit and damage');\n",
    "plt.xlabel(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot matrix (might take a while)\n",
    "plot_cols = ['outcome_damage_inc', 'income_am', 'profit_last_am', 'profit_am', 'damage_am', 'damage_inc', 'crd_lim_rec']\n",
    "sns.pairplot(train_V2[plot_cols], height=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "1. the revenue per client (= profit - damage)\n",
    "    - needs to be calculated\n",
    "2. predict which clients will cause damage\n",
    "    - outcome_damage_inc\n",
    "3. predict the amount of damage fot those who will cause damage / wreak havoc\n",
    "    - outcome_damage_amount\n",
    "\n",
    "Calculate revenue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- es trainen op apart amount profit & damage, maar ook es op verschil (revenue)\n",
    "- testen of weglaten van de outliers iets oplevert\n",
    "\n",
    "## verdeling\n",
    "\n",
    "- Bino : GBM\n",
    "- Marijke : RF\n",
    "- Dries : SVR/SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Booster Regression\n",
    "\n",
    "In this section we will attempt to use a gradient booster regression to explain the profit & damage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingRegressor().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a train test split with a 20 % holdout for final evaluation, we will use K-fold cross-validation on the train set. Since here we are applying a tree based method, no rescaling is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(target_features, axis=1), \n",
    "                                                    train_V2[\"outcome_profit\"],  \n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we play around manually with the GradientBoostingRegressor parameters. It was noticed that\n",
    "- including a limit to the `max_leaf_nodes` does increase test R2, so will include this in hyperparam search\n",
    "- we have to choose the `n_estimators` (# boosting stages) fairly large it seems, while still keeping learning rate to relatively high value, strange, will optimize further in the hyperparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params = {\n",
    "    'n_estimators': 600,  # number of boosting stages to perform\n",
    "    'subsample': 1.0,     # 1.0 leads to reduction in variance and increase in bias\n",
    "    'max_depth': 3,       # limiting the max depth to the default of 3 seems to improve the test R2\n",
    "    'min_samples_split': 2, # default\n",
    "    'min_samples_leaf': 1,  # default\n",
    "    'max_leaf_nodes': 8, # limiting the max number of leaf nodes does increase performance !! improvement of test R2 vs using None\n",
    "    'learning_rate': 0.1, # default\n",
    "    'ccp_alpha': 0.0}  # default is 0.0\n",
    "\n",
    "gbm = GradientBoostingRegressor(loss='squared_error', random_state=seed, **gbm_params) \n",
    "gbm.fit(X=X_train, y=y_train) \n",
    "\n",
    "# Step 4 - Getting predictions\n",
    "gbm_preds = gbm.predict(X=X_test)\n",
    "gbm_preds_train = gbm.predict(X=X_train)\n",
    "\n",
    "test_R2 = gbm.score(X=X_test, y=y_test)\n",
    "train_R2 = gbm.score(X=X_train, y=y_train)\n",
    "\n",
    "# Print R2 results\n",
    "print(f'Train R2: {train_R2:.3}')\n",
    "print(f'Test R2: {test_R2:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed a few things : \n",
    "- leaving out the 2 outliers in the `outcome_profit` of > 25000 results in a significant increase in Test R2 (0.85 vs. 0.81), so perhaps it is better to leave those values out of the training as it is not certain how representave they are for the final scoring dataset. We can revisit this argument after the hyperparameter scan. \n",
    "- we also tried log-transforming the `outcome_profit` feature before fitting (`np.log(1+train_V2[\"outcome_profit\"])`) in order to test whether this would give any improvement, given the fairly skew distribution of the `outcome_profit`values. This resulting in drastically lower test R2 (and also train R2) of 0.65 and 0.86 respectively, so we will not apply any log-transformation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- let's make a quick plot of the results for the training & test dataset\n",
    "axis_range = [0, 25000]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axs[0].plot(gbm_preds_train, y_train, '.')\n",
    "axs[0].set_title(f\"Training dataset (Train R2 = {train_R2:.3})\")\n",
    "axs[0].set_ylim(axis_range)\n",
    "axs[0].set_xlim(axis_range)\n",
    "\n",
    "axs[1].plot(gbm_preds, y_test, '.')\n",
    "axs[1].set_title(f\"Test dataset (Test R2 = {test_R2:.3})\")\n",
    "axs[1].set_ylim(axis_range)\n",
    "axs[1].set_xlim(axis_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to optimise the manually tweaked hyperparameter further in an automated way using a RandomisedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- first chooe how many hyper parameters to randomly sample\n",
    "n_hyperparam_sample = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting them all in a dictionary\n",
    "random_grid = {\n",
    "    'n_estimators': [ int(x) for x in np.linspace(start=200, stop=1000, num=9)],\n",
    "    'max_depth': [int(x) for x in np.linspace(1, 10, num=10)],\n",
    "    'min_samples_split': [2, 5, 10, 30],\n",
    "    'min_samples_leaf': [1, 2, 4, 10, 30],\n",
    "    'max_leaf_nodes': [int(x) for x in np.linspace(2, 20, num=19)],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.08, 0.1],\n",
    "    }\n",
    "\n",
    "# currently leave out those : \n",
    "#'max_features':  [0.1, 0.3, 0.5, 0.7, 0.9] # not using this one... best leave at default !\n",
    "#'subsample': [0.4, 0.6, 0.8, 1],\n",
    "\n",
    "\n",
    "#min_impurity_decrease: 0.0,\n",
    "# Instantiate base model to tune (RF with default settings, altough you can fix parameters here if you want)\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, across 500 different hyperparameter combinations, and use all available cores\n",
    "rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=gbr, \n",
    "                                                 param_distributions=random_grid, \n",
    "                                                 n_iter=n_hyperparam_sample,   # Amount of hyperparameter values to sample\n",
    "                                                 cv=KFold(n_splits=5, shuffle=True, random_state=seed), # setting the shuffle to True here ... \n",
    "                                                 verbose=2, # to print some outputs\n",
    "                                                 random_state=seed, \n",
    "                                                 n_jobs=-1)\n",
    "\n",
    "# Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Now you can access the fitted object, e.g. get best_params_\n",
    "print(f\"Optimally found RF hyperparams after this random search: {rf_hyperparam_tuning_random.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "### Model to predict revenue per client \n",
    "\n",
    "Goal of the What do we need to predict? \n",
    "1. the revenue per client (= profit - damage)\n",
    "\n",
    "We could fit the revenue by fitting the profit and by fitting the damage and subtracting them. Or we could calculate the revenue in the training set and fit the revenue as such.\n",
    "\n",
    "When you need to predict a difference, you would typically predict each term individually and then calculate the difference between the predicted values. By predicting each term separately, you can estimate the values of the components involved in the difference and then determine the overall difference based on these predictions. This approach helps in forecasting the outcome of the difference by considering the predicted values of its individual components.\n",
    "\n",
    "Below we have looked at the R2 result for an RF-model on the revenue and an RF-model on the profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome to maximize is profit - damage\n",
    "\n",
    "# don't assign to dataframe just yet ???\n",
    "train_V2['revenue'] = train_V2['outcome_profit'] - train_V2['outcome_damage_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split and selecting target = outcome_profit\n",
    "#\n",
    "# A train split of 20% is chosen to balance between having enough data to train the model effectively and having a sufficient amount of data to evaluate its performance.\n",
    "#   With a larger training set (80%), the model can learn more patterns and relationships in the data, potentially leading to better performance.\n",
    "#   On the other hand, a smaller test set (20%) allows for a more robust evaluation of the model's generalization capabilities.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue'], axis=1), # features DF\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue','score_pos','score_neg','not_null_pos_columns','not_null_neg_columns'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_profit'],   # target DF/series\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,  \n",
    "                                                    random_state=seed)  \n",
    "\n",
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "n_estimators = [int(x) for x in np.linspace(start=150, stop=300, num=6)]  # list comprehension because we want integers!\n",
    "max_depth = [int(x) for x in np.linspace(start=5, stop=85, num=8)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "max_features = [0.7, 0.8, 0.9, 0.98]\n",
    "min_samples_split = [4, 5, 6, 8]\n",
    "min_samples_leaf = [2, 3, 4,6]\n",
    "bootstrap = [True]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "# outcome_profit is continuous, therefore a regressor needs to be applied\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=rfr, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "print('Optimally found RF hyperparams after this random search: \\n{}' .format(rf_hyperparam_tuning_random.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_profit_best = rf_hyperparam_tuning_random.best_estimator_\n",
    "rf_profit_best.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_profit_best_y_train_pred = rf_profit_best.predict(X=X_train)\n",
    "rf_profit_best_y_test_pred = rf_profit_best.predict(X=X_test)\n",
    "\n",
    "# Checking the score: R2 manually\n",
    "print('Train R2: %.3f' % rf_profit_best.score(X=X_train, y=y_train))\n",
    "print('Test R2: %.3f' % rf_profit_best.score(X=X_test, y=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_profit_best, open('RF_model_profit.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning_random.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 'param_n_estimators', 'param_max_features']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV R2')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we see that not all params in the RanomizedGridSearch seem to have an impact on the performance of the RandomForestRegressor.  \n",
    "However performances gets better for trees with a higher depth.  \n",
    "The model also shows better performance for a higher percentage for max_features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting hyperparameter vs. performance: more complex plots\n",
    "plt.scatter(x=rf_cv_res['param_max_depth'], \n",
    "            y=rf_cv_res['mean_test_score'], \n",
    "            c=rf_cv_res['param_max_features'], # c argument allows to color according to param_n_estimators\n",
    "            cmap='plasma');  # This makes the color non-dull\n",
    "plt.colorbar();  # adds the bar on the right\n",
    "plt.xlabel('param_max_depth');\n",
    "plt.ylabel('CV R2');\n",
    "plt.title('Max depth (x) and max features (color) vs. CV R2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning_curve for our mode\n",
    "cuf.plot_learning_curve(model=rf_profit_best, \n",
    "                    X=train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue'], axis=1), \n",
    "                    y=train_V2['outcome_profit'], \n",
    "                    cv=5, \n",
    "                    num_show=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict the amount of damage per client\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data?\n",
    "\n",
    "3. predict the amount of damage fot those who will cause damage / wreak havoc\n",
    "    - outcome_damage_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split (Note: we're taking 30% test set size here instead of 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','revenue','outcome_profit'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_damage_amount'],   # target DF/series\n",
    "                                                    test_size=0.3, # 30% as test or validation set (who cares about the exact names)\n",
    "                                                    shuffle=True,  # This shuffles the data! (Important)\n",
    "                                                    random_state=seed)  # setting seed for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "# n_estimators = [int(x) for x in np.linspace(start=100, stop=300, num=10)]  # list comprehension because we want integers!\n",
    "# max_features = [int(x) for x in np.linspace(start=5, stop=50, num=8)] \n",
    "# max_depth = [int(x) for x in np.linspace(start=2, stop=100, num=10)]\n",
    "# max_depth.append(None)  # adding 'None' option as well\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4, 10]\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=150, stop=300, num=6)]  # list comprehension because we want integers!\n",
    "max_depth = [int(x) for x in np.linspace(start=5, stop=85, num=8)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "max_features = [0.7, 0.8, 0.9, 0.98]\n",
    "min_samples_split = [4, 5, 6, 8]\n",
    "min_samples_leaf = [2, 3, 4,6]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "print('Optimally found RF hyperparams after this random search: \\n{}' .format(rf_hyperparam_tuning_random.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_damageAmount = rf_hyperparam_tuning_random.best_estimator_\n",
    "rf_damageAmount.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_damageAmount_best_y_train_pred = rf_damageAmount.predict(X=X_train)\n",
    "rf_damageAmount_best_y_test_pred = rf_damageAmount.predict(X=X_test)\n",
    "\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train R2: %.3f' % rf_damageAmount.score(X=X_train, y=y_train))\n",
    "print('Test R2: %.3f' % rf_damageAmount.score(X=X_test, y=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_damageAmount, open('RF_model_damage_amount.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning_random.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_leaf', 'param_n_estimators']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV R2')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict which clients will cause damage\n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "\n",
    "2. predict which clients will cause damage\n",
    "    - outcome_damage_inc\n",
    "\n",
    "\n",
    "Train-test set split for output = output_damage_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making train-test set split (Note: we're taking 30% test set size here instead of 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','revenue','outcome_profit'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_damage_inc'],   # target DF/series\n",
    "                                                    test_size=0.4, # 30% as test or validation set (who cares about the exact names)\n",
    "                                                    shuffle=True,  # This shuffles the data! (Important)\n",
    "                                                    random_state=seed)  # setting seed for consistent results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "# n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=5)]\n",
    "# max_features = [int(x) for x in np.linspace(start=5, stop=150, num=20)] \n",
    "# max_depth = [int(x) for x in np.linspace(start=2, stop=40, num=10)]\n",
    "# max_depth.append(None)  # adding 'None' option as well\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4, 10]\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=150, stop=300, num=6)] \n",
    "max_depth = [int(x) for x in np.linspace(start=5, stop=85, num=8)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "max_features = [0.7, 0.8, 0.9, 0.98]\n",
    "min_samples_split = [4, 5, 6, 8]\n",
    "min_samples_leaf = [2, 3, 4,6]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning = RandomizedSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          n_iter=n_hyperparams_sample, \n",
    "                                          cv=kfold,\n",
    "                                          verbose=2, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Executing / fitting the random search \n",
    "rf_hyperparam_tuning.fit(X=X_train, y=y_train)\n",
    "print('Optimal hyperparameter values according to our random search: \\n{}' .format(rf_hyperparam_tuning.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_damageInd = rf_hyperparam_tuning.best_estimator_\n",
    "rf_damageInd.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_damageInd_y_train_pred = rf_damageInd.predict(X=X_train)\n",
    "rf_damageInd_y_test_pred = rf_damageInd.predict(X=X_test)\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train accuracy of the refitted model: %.3f' % rf_damageInd.score(X=X_train, y=y_train))\n",
    "print('Test accuracy of the refitted model: %.3f' % rf_damageInd.score(X=X_test, y=y_test))\n",
    "\n",
    "# Classification report \n",
    "target_names = ['no damage', 'damage']\n",
    "print(classification_report(y_true=y_test, y_pred=rf_damageInd_y_test_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_damageInd, open('..\\\\RF_model_damage_inc.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Setting Decision Threshold\n",
    "\n",
    "For models that output a continuous probability-like value (basically: most models except SVM), the predict function really just applies a cut-off of 0.5 to the predicted values returned from the model. In cases like this where there is class imbalance, probabilities will only start to deviate from the imbalanced 30% once the model has good separation of classes. In the absence of that, 0.5 is definitely not the best cutoff. This leads to e.g. a recall of 0.0. You will in fact see that many of the algorithms in this practical get both precision and recall of the expensive class equal to 0, which means that no instance is classified as expensive with the default cutoff of 0.5.\n",
    "\n",
    "Let's try the same, but with another cutoff, e.g. 0.3 and check it out on our tuned random forest model rf_opt1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting hyperparameter tuning results and checking\n",
    "rf_cv_res = pd.DataFrame(rf_hyperparam_tuning.cv_results_)\n",
    "\n",
    "# Scatter plot of selection of hyperparams vs performance\n",
    "plot_hyperparams = ['param_max_depth', 'param_min_samples_leaf', 'param_n_estimators']  \n",
    "# in the .cv_results_ there is always a 'param_' prefix!\n",
    "\n",
    "for param in plot_hyperparams:\n",
    "    plt.figure(); # This command will help in the plots not 'falling' on top of each other ;)\n",
    "    plt.scatter(x=rf_cv_res[param], y=rf_cv_res['mean_test_score'], alpha=0.7); # alpha to get some opacity\n",
    "    plt.xlabel(param);\n",
    "    plt.ylabel('CV Accuracy')\n",
    "    plt.title(f\"Hyperparameter: {param} vs. CV Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of damage\n",
    "\n",
    "\n",
    "Setting Decision Threshold\n",
    "\n",
    "In the context of our problem we want to select the applicants that will generate the highest revenue. Whether they do or do not cause damage is less important, therefore we will look at precision rather than recall. Precision measures the accuracy of the selected cases, specifically the proportion of selected cases that are relevant (in this case, generating high revenue).\n",
    "\n",
    "For models that output a continuous probability-like value (basically: most models except SVM), the predict function really just applies a cut-off of 0.5 to the predicted values returned from the model. In cases like this where there is class imbalance, probabilities will only start to deviate from the imbalanced 30% once the model has good separation of classes. In the absence of that, 0.5 is definitely not the best cutoff. This leads to e.g. a recall of 0.0. You will in fact see that many of the algorithms in this practical get both precision and recall of the expensive class equal to 0, which means that no instance is classified as expensive with the default cutoff of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision cutoff/threshold tuning for classification models\n",
    "# Choose a cutoff/threshold value\n",
    "cutoff = 0.15\n",
    "\n",
    "# Refitting an RF with the tuned hyperparameters from the previous cell\n",
    "print(f\"Checking parameters of rf_damageInd: {rf_damageInd}\")\n",
    "\n",
    "# Getting performances & classification report\n",
    "print('Train accuracy: %.3f' % rf_damageInd.score(X=X_train, y=y_train))\n",
    "print('Test accuracy: %.3f' % rf_damageInd.score(X=X_test, y=y_test))\n",
    "target_names = ['No damage', 'Damage']\n",
    "print(classification_report(y_test, (rf_damageInd.predict_proba(X=X_test)[:, 1] > cutoff), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECALL:\n",
    " It measures the ability of a model to correctly identify all relevant instances within a dataset. Mathematically, recall is calculated as the number of true positive predictions divided by the sum of true positives and false negatives. It is also known as sensitivity or true positive rate. A high recall value indicates that the model is good at identifying all positive instances in the dataset.\n",
    "\n",
    "PRECISION\n",
    " It measures the proportion of true positive predictions out of all positive predictions made by the model. Mathematically, precision is calculated as the number of true positive predictions divided by the sum of true positives and false positives. A high precision value indicates that when the model predicts a positive result, it is likely to be correct.\n",
    "\n",
    "the F1 score is a metric that combines both precision and recall into a single value. It is calculated as the harmonic mean of precision and recall. The F1 score provides a balance between precision and recall, giving equal weight to both metrics. A high F1 score indicates that the model has both good precision and recall, making it a useful metric for evaluating the overall performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Confusion matrix for random forest\n",
    "rf_damage_pred_proba = rf_damageInd.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# If you sum per row you just get 1, to know which proability is what, they are ordered as in the classes_ attribute\n",
    "print(f\"Ordering of the target classes: {rf_damageInd.classes_}\")  # So False for col 0, True for col1!\n",
    "\n",
    "# cleaner since col 2 is 1 - col1:\n",
    "rf_damageInd_confusion_matrix = confusion_matrix(y_true=y_test, y_pred=rf_damageInd_y_test_pred)\n",
    "print(rf_damageInd_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision cutoff vs. precision\n",
    "# Making a grid of potential cutoff values (between 0, 1)\n",
    "cutoffs = np.arange(1, 101) / 100  # Range of cutoffs\n",
    "\n",
    "# Getting the test precisions with  list comprehension (could be a loop as well)\n",
    "precisions = [y_test[rf_damage_pred_proba[:,1] > c].mean() for c in cutoffs]\n",
    "#[rf_damageInd.predict_proba(X=X_test)[:,1] > c].mean() is a way to get the proportion of positively predicted values\n",
    "\n",
    "# Plotts\n",
    "plt.figure();\n",
    "plt.plot(cutoffs, precisions);\n",
    "plt.xlabel('Cutoff');\n",
    "plt.ylabel('Precision (PPV)');\n",
    "plt.title('Potential cutoffs versus resulting test precisions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration\n",
    "\n",
    "To assess the calibration of a Random Forest Classifier and determine if the predicted probabilities are accurate, you can use a calibration curve. This curve compares the predicted probabilities to the actual outcomes. By plotting the predicted probabilities against the true probabilities, you can visually inspect the calibration of your model.\n",
    "\n",
    "By examining this calibration curve, you can determine if the predicted probabilities align well with the actual outcomes. If the curve is close to the diagonal line (perfect calibration), it indicates that the model's predicted probabilities are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve \n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "CalibrationDisplay.from_estimator(estimator=rf_damageInd,\n",
    "                                  X=X_test, \n",
    "                                  y=y_test,\n",
    "                                  n_bins=5,\n",
    "                                  strategy='uniform');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the probability is close to the measured probability for the lower probabilities. For higher probabilities there is a lack of fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrating a RF model\n",
    "# Importing necessary classes and function\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# Doing isotonic regression on our RF model\n",
    "rf_damageInd_isotonic = CalibratedClassifierCV(estimator=rf_damageInd, \n",
    "                                          cv='prefit', \n",
    "                                          method='isotonic')\n",
    "rf_damageInd_isotonic.fit(X=X_test, y=y_test)\n",
    "\n",
    "# Checking calibration curve on the calibration set\n",
    "y_test_calibrated = rf_damageInd_isotonic.predict_proba(X=X_test)[:, 1]\n",
    "\n",
    "# Plotting calibration curves after isotonic regression calibration\n",
    "# skplt.metrics.plot_calibration_curve(y_true=y_test, probas_list=[y_test_calibrated], n_bins=10);\n",
    "# plt.title('Calibration Curve on test set - CALIBRATED');\n",
    "### ERROR on gctmap attribute in matplotlib? module 'matplotlib.cm' has no attribute 'get_cmap'\n",
    "\n",
    "# Alternative way of plotting\n",
    "first, second = calibration_curve(y_true=y_test, y_prob=y_test_calibrated, n_bins=20)  # Returns 2 arrays\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(first, second, marker='.')\n",
    "plt.title('Calibration Curve on test set - CALIBRATED')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve after isotonic regression\n",
    "y_pred_train_calibrated = rf_damageInd_isotonic.predict_proba(X=X_train)[:, 1]\n",
    "y_pred_train_uncalibrated = rf_damageInd.predict_proba(X=X_train)[:, 1]\n",
    "\n",
    "# Plotting calibration curves after isotonic regression calibration\n",
    "# skplt.metrics.plot_calibration_curve(y_true=y_train, probas_list=[y_pred_train_calibrated], n_bins=10);\n",
    "# plt.title('Calibration Curve on train set with calibrated model');\n",
    "\n",
    "# Plotting calibration curves after isotonic regression calibration\n",
    "# skplt.metrics.plot_calibration_curve(y_true=y_train, probas_list=[y_pred_train_uncalibrated], n_bins=10);\n",
    "# plt.title('Calibration Curve on train set with raw model');\n",
    "\n",
    "first, second = calibration_curve(y_true=y_train, y_prob=y_pred_train_calibrated, n_bins=10)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', alpha=0.9, label='Perfect calibration')\n",
    "plt.plot(first, second, marker='.', label='Model');\n",
    "plt.legend();\n",
    "plt.title('Calibration Curve on train set with calibrated model');\n",
    "plt.show()\n",
    "\n",
    "first, second = calibration_curve(y_true=y_train, y_prob=y_pred_train_uncalibrated, n_bins=10)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', alpha=0.9, label='Perfect calibration')\n",
    "plt.plot(first, second, marker='.', label='Model');\n",
    "plt.legend();\n",
    "plt.title('Calibration Curve on train set with calibrated model');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lift curve\n",
    "\n",
    "For us, the lift curve is an interesting tool, as it shows how much benefit the model has when you use it for selection. Typically, the cutoff is also made by assessing the two quantities shown on the lift curve: either one cuts the list off at a certain amount of benefit (\"this will make my list that much purer\"), or one cuts it off at a desired sample (\"I'm able to contact 300 clients, which coincides with 10% of the population\"). Note that the first logic is slightly suspect: usually there is a discrepancy between how you got the data, and how your future actions will work (\"my data are from an administrative data base, and now I will use an email to contact new clients\"), which means that the exact numbers are to be taken with a grain of salt. The second logic is better: if you have to contact 300 clients, these are probably the 300 best you can find.\n",
    "\n",
    "The lift curve (or cumulative gains curve) is a common tool to talk about the strength of your lead list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you sum per row you just get 1, to know which proability is what, they are ordered as in the classes_ attribute\n",
    "print(f\"Ordering of the target classes: {rf_damageInd.classes_}\")  # So False for col 0, True for col1!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- berekende classification lift\n",
    "cuf.classification_lift(y_test.values,np.where(rf_damage_pred_proba[:,1]< 0.5, 0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- cumulatieve lift\n",
    "cumulative_lift = cuf.cumulative_classification_lift(y_test.values,np.where(rf_damage_pred_proba[:,1]< 0.5, 0,1))\n",
    "type(cumulative_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Lift curve for clients causing damage ((rf_damage_pred_proba[:,1])\n",
    "# \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, len(cumulative_lift) + 1) / len(cumulative_lift), cumulative_lift, marker='o', linestyle='-', color='b')\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "plt.xlabel('Proportion of Samples')\n",
    "plt.ylabel('Cumulative Lift')\n",
    "plt.title('Cumulative Lift Curve')\n",
    "plt.legend(['Model', 'Random'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to predict revenue per client \n",
    "\n",
    "Goal of the What do we need to predict? Is it available as outcome in our data\n",
    "1. the revenue per client (= profit - damage)\n",
    "\n",
    "We could fit the revenue by fitting the profit and by fitting the damage and subtracting them. Or we could calculate the revenue in the training set and fit the revenue as such.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making train-test set split and select the target = revenue\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue'], axis=1), # features DF\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue','score_pos','score_neg','not_null_pos_columns','not_null_neg_columns'], axis=1), # features DF\n",
    "#                                                     train_V2['revenue'],   # target DF/series\n",
    "#                                                     test_size=0.2, \n",
    "#                                                     shuffle=True,  \n",
    "#                                                     random_state=seed)  \n",
    "\n",
    "\n",
    "# # Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "# n_estimators = [int(x) for x in np.linspace(start=150, stop=300, num=10)]  # list comprehension because we want integers!\n",
    "# max_depth = [int(x) for x in np.linspace(start=5, stop=80, num=5)]\n",
    "# max_depth.append(None)  # adding 'None' option as well\n",
    "# max_features = [0.7, 0.8, 0.85, 0.9, 0.95]\n",
    "# min_samples_split = [4, 5, 6, 8]\n",
    "# min_samples_leaf = [2, 3, 4]\n",
    "# bootstrap = [True]\n",
    "# hyperparam_grid = {'n_estimators': n_estimators,\n",
    "#                    'max_features': max_features,\n",
    "#                    'max_depth': max_depth,\n",
    "#                    'min_samples_split': min_samples_split,\n",
    "#                    'min_samples_leaf': min_samples_leaf,\n",
    "#                    'bootstrap': bootstrap}\n",
    "\n",
    "# # Define data splitter to be used in the search\n",
    "# n_folds = 5\n",
    "# kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# # Define amount of hyperparameter tuning combinations to sample\n",
    "# n_hyperparams_sample = 100\n",
    "\n",
    "# # Defining model to apply random search CV hyperparam tuning on\n",
    "# rf = RandomForestRegressor()\n",
    "\n",
    "# # Initializing random search CV object\n",
    "# rf_hyperparam_tuning_random = RandomizedSearchCV(estimator=rf, \n",
    "#                                           param_distributions=hyperparam_grid, \n",
    "#                                           n_iter=n_hyperparams_sample, \n",
    "#                                           cv=kfold,\n",
    "#                                           verbose=2, \n",
    "#                                           random_state=seed, \n",
    "#                                           n_jobs=-1)\n",
    "\n",
    "# # Fit the random search by sampling hyperparameters from our grid, then fitting each model for each CV fold, aggregating results\n",
    "# rf_hyperparam_tuning_random.fit(X=X_train, y=y_train)\n",
    "\n",
    "# print('Optimally found RF hyperparams after this random search: \\n{}' .format(rf_hyperparam_tuning_random.best_params_))\n",
    "\n",
    "# # Refitting the optimal model on the whole training dataset\n",
    "# rf_revenue_best = rf_hyperparam_tuning_random.best_estimator_\n",
    "# rf_revenue_best.fit(X=X_train, y=y_train)\n",
    "\n",
    "# # Getting predictions on train and test set\n",
    "# rf_revenue_best_y_train_pred = rf_revenue_best.predict(X=X_train)\n",
    "# rf_revenue_best_y_test_pred = rf_revenue_best.predict(X=X_test)\n",
    "\n",
    "# # Checking score (R2) manually\n",
    "# print('Train R2: %.3f' % rf_revenue_best.score(X=X_train, y=y_train))\n",
    "# print('Test R2: %.3f' % rf_revenue_best.score(X=X_test, y=y_test))\n",
    "# pickle.dump(rf_revenue_best, open('..\\\\RF_model_revenue.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HalvingRandomSearchCV: Randomized search on hyper parameters (experimental ).\n",
    "\n",
    "The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n",
    "\n",
    "The candidates are sampled at random from the parameter space and the number of sampled candidates is determined by n_candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Defining candidate grid to sample from (RandomizedSearchCV will sample from it)\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=500, num=5)]  # list comprehension because we want integers!\n",
    "max_features = [int(x) for x in np.linspace(start=5, stop=150, num=20)] \n",
    "max_depth = [int(x) for x in np.linspace(start=2, stop=40, num=10)]\n",
    "max_depth.append(None)  # adding 'None' option as well\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "hyperparam_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "# Define data splitter to be used in the search\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define amount of hyperparameter tuning combinations to sample\n",
    "n_hyperparams_sample = 100\n",
    "\n",
    "# Defining model to apply random search CV hyperparam tuning on\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initializing random search CV object\n",
    "rf_hyperparam_tuning = HalvingRandomSearchCV(estimator=rf, \n",
    "                                          param_distributions=hyperparam_grid, \n",
    "                                          factor=3,\n",
    "                                          cv=kfold,\n",
    "                                          verbose=1, \n",
    "                                          random_state=seed, \n",
    "                                          n_jobs=-1)\n",
    "\n",
    "# Executing / fitting the random search \n",
    "rf_hyperparam_tuning.fit(X=X_train, y=y_train)\n",
    "print('Optimal hyperparameter values according to our random search: \\n{}' .format(rf_hyperparam_tuning.best_params_))\n",
    "\n",
    "# Refitting the optimal model on the whole training dataset\n",
    "rf_damageInd_hrs = rf_hyperparam_tuning.best_estimator_\n",
    "rf_damageInd_hrs.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Getting predictions on train and test set\n",
    "rf_damageInd_hrs_y_train_pred = rf_damageInd_hrs.predict(X=X_train)\n",
    "rf_damageInd_hrs_y_test_pred = rf_damageInd_hrs.predict(X=X_test)\n",
    "\n",
    "# Checking accuracy manually\n",
    "print('Train accuracy of the refitted model: %.3f' % rf_damageInd_hrs.score(X=X_train, y=y_train))\n",
    "print('Test accuracy of the refitted model: %.3f' % rf_damageInd_hrs.score(X=X_test, y=y_test))\n",
    "\n",
    "# Classification report \n",
    "target_names = ['no damage', 'damage']\n",
    "print(classification_report(y_true=y_test, y_pred=rf_damageInd_hrs_y_test_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue'], axis=1), # features DF\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue','score_pos','score_neg','not_null_pos_columns','not_null_neg_columns'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_profit'],   # target DF/series\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,  \n",
    "                                                    random_state=seed)  \n",
    "\n",
    "# Flexible kNN regression on high-dimensional data : hyperparameter tuning > lower k \n",
    "\n",
    "n_neighbors = np.arange(1, 30)\n",
    "hyperparam_grid = {'n_neighbors': n_neighbors} # GridSearchCV wants a dictionary ;)\n",
    "\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Instantiate the model of choice\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Instantiate the GridSearchCV object with our hyperparameter grid, amount of folds, ...\n",
    "knn_hyperparam_tuning = GridSearchCV(estimator=knn, \n",
    "                                     param_grid=hyperparam_grid, \n",
    "                                     cv=kfold,  # supply the kfold object to the Grid search hyperparam tuning \n",
    "                                     verbose=2, \n",
    "                                     n_jobs=-1)  # For parallel computing on all available threads\n",
    "\n",
    "# Carry out / fit using our data\n",
    "knn_hyperparam_tuning.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Extracting best parameters\n",
    "print(f'Optimal hyperparameters according to our grid search: {knn_hyperparam_tuning.best_params_}')\n",
    "print('Train R2: %.3f' % knn_hyperparam_tuning.best_estimator_.score(X=X_train, y=y_train))\n",
    "print('Test R2: %.3f' % knn_hyperparam_tuning.best_estimator_.score(X=X_test, y=y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test R2: %.3f' % knn_hyperparam_tuning.best_estimator_.score(X=X_test, y=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking cross-validation results and visualizing\n",
    "# Putting CV results in a nice Pandas DataFrame\n",
    "knn_cv_results = pd.DataFrame(knn_hyperparam_tuning.cv_results_)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x=knn_cv_results['param_n_neighbors'], y=knn_cv_results['mean_test_score']);\n",
    "plt.title('Amount of neighbors (k) vs. CV R2 (Zoomed out)');\n",
    "plt.xlabel('Amount of neighbors (k)');\n",
    "plt.ylabel('CV R2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking cross-validation results and visualizing\n",
    "# Putting CV results in a nice Pandas DataFrame\n",
    "knn_cv_results = pd.DataFrame(knn_hyperparam_tuning.cv_results_)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x=knn_cv_results['param_n_neighbors'], y=knn_cv_results['mean_test_score']);\n",
    "plt.title('Amount of neighbors (k) vs. CV R2 (Zoomed out)');\n",
    "plt.xlabel('Amount of neighbors (k)');\n",
    "plt.ylabel('CV R2');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiteboxing and explainability\n",
    "\n",
    "Because we see that the RF model for predicting profit performs better we will use this to investigate:\n",
    "- which features matter?  \n",
    "- what is the influence of a feature?  \n",
    "- why is the prediction what it is?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results and not having to run the cross validation repeatedly the selected models were saved and are reloaded below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_profit_best = pickle.load(open('..\\\\RF_model_profit.pkl','rb'),encoding='utf-8')\n",
    "rf_damage_amount_best = pickle.load(open('..\\\\RF_model_damage_amount.pkl','rb'),encoding='utf-8')\n",
    "rf_damage_inc_best = pickle.load(open('..\\\\RF_model_damage_inc.pkl','rb'),encoding='utf-8')\n",
    "rf_damage_inc_hrs_best = pickle.load(open('..\\\\RF_model_damage_inc_hrs.pkl','rb'),encoding='utf-8')\n",
    "#rf_revenue_best = pickle.load(open('..\\\\RF_model_revenue.pkl','rb'),encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Needed for explainability: Mind to use the correct target for the corresponding model\n",
    "\n",
    "train_V2 = pd.read_csv('..\\\\train_V2_prep.csv')\n",
    "score = pd.read_csv('..\\\\score_prep.csv')\n",
    "rf_profit_best = pickle.load(open('.\\\\RF_model_profit.pkl','rb'),encoding='utf-8')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue'], axis=1), # features DF\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_V2.drop(['outcome_damage_amount','outcome_damage_inc','outcome_profit','revenue','score_pos','score_neg','not_null_pos_columns','not_null_neg_columns'], axis=1), # features DF\n",
    "                                                    train_V2['outcome_profit'],   # target DF/series\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,  \n",
    "                                                    random_state=seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based (global) feature importances (which features matter?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances of the fitted rf1 model (looking at first 10 rows)\n",
    "rf_profit_best.feature_importances_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding which features or variables have the most impact on the model's predictions\n",
    "# Feature importance in a Random Forest model is determined by how much each feature contributes to decreasing the impurity in the nodes of the trees within the forest.\n",
    "# rf_profit_best.feature_importances_[0:10]\n",
    "\n",
    "# Putting feature importances of our random forest model into a DataFrame and show the 10 most important features\n",
    "rf_profit_best = (pd.DataFrame({'feature': X_train.columns, \n",
    "                        'feature_importance': rf_profit_best.feature_importances_})\n",
    "          .sort_values(by='feature_importance', ascending=False)\n",
    "          .reset_index(drop=True)) # resetting index\n",
    "\n",
    "rf_profit_best.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top k feature importances\n",
    "# Extracting top k\n",
    "_k = 20\n",
    "rf_profit_best_top_k = rf_profit_best.head(_k).sort_values(by='feature_importance', ascending=True)\n",
    "\n",
    "# Plotting\n",
    "plt.barh(y=rf_profit_best_top_k['feature'], width=rf_profit_best_top_k['feature_importance'], color='deepskyblue'); # pretty colors wow\n",
    "plt.xlabel('Scaled feature importance');\n",
    "plt.ylabel('Feature');\n",
    "plt.title(f'Top {_k} feature importance for model rf1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation feature importances - this will take a while (~3 min)\n",
    "rf_profit_best_fi_perm = permutation_importance(estimator=rf_profit_best, \n",
    "                                     X=X_train, \n",
    "                                     y=y_train, \n",
    "                                     n_repeats=5,  # 5 permutation runs! \n",
    "                                     random_state=seed)\n",
    "\n",
    "# Converting to a Pandas DataFrame & sorting\n",
    "rf_profit_best_fi_perm_df = (pd.DataFrame({'feature': X_train.columns,\n",
    "                               'importances_mean': rf_profit_best_fi_perm['importances_mean'],\n",
    "                               'importances_std': rf_profit_best_fi_perm['importances_std']})\n",
    "                  .sort_values(by='importances_mean', ascending=False)\n",
    "                  .reset_index(drop=True))\n",
    "rf_profit_best_fi_perm_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 20 result again, with error bars\n",
    "# Getting top k\n",
    "_k = 20\n",
    "rf_profit_best_df_top_k = (rf_profit_best_fi_perm_df\n",
    "                        .head(_k) # take top 20\n",
    "                        .sort_values(by='importances_mean', ascending=True))\n",
    "\n",
    "# Plotting with error bars\n",
    "plt.barh(y=rf_profit_best_df_top_k['feature'], \n",
    "         width=rf_profit_best_df_top_k['importances_mean'], # size of the bar is the avg. importance\n",
    "         xerr=rf_profit_best_df_top_k['importances_std'],  # error bar is the standard deviation\n",
    "         color='deepskyblue');  # For error bars, handy!\n",
    "plt.xlabel('Permutation-based scaled feature importances');\n",
    "plt.ylabel('Feature');\n",
    "plt.title('Permutation-based feature importances for model rf1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- drop-column feature importances: assessing the impact or importance of a specific feature (column) in a machine learning model.\n",
    "#   By dropping the column and observing the change in model performance, you can gauge the significance of that particular feature in predicting the target variable.\n",
    "rf_profit_best_importances_drop = cuf.drop_col_feat_imp(model=rf_profit_best, \n",
    "                                         X=X_train, \n",
    "                                         y=y_train, \n",
    "                                         random_state=seed)\n",
    "\n",
    "rf_profit_best_importances_drop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting top k of drop-column feature importances\n",
    "_k = 20\n",
    "rf_profit_best_importances_drop_top_k = (rf_profit_best_importances_drop.sort_values(by='importance', ascending=False)\n",
    "                             .head(_k)\n",
    "                             .sort_values(by='importance', ascending=True)) # To get it to plot nicely (sort WITHIN top _k)\n",
    "\n",
    "# Plotting\n",
    "plt.barh(y=rf_profit_best_importances_drop_top_k ['feature'], \n",
    "         width=rf_profit_best_importances_drop_top_k['importance'], \n",
    "         color='deepskyblue', \n",
    "         ecolor='black');\n",
    "plt.title('Drop-column method feature importances')\n",
    "plt.xlabel('Importance');\n",
    "plt.ylabel('Feature');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear surrogate explainability models (what is the influence of a feature)\n",
    "\n",
    "Feature importances only tell us one thing for each feature: was it important for the whole model under consideration or not? There are other questions to answer: exactly how are the features related to the target variable, which direction and what magnitude? To this end, we build a surrogate regression model that uses the predicted value from the model you want to explain as a target. Obviously, we want this surrogate model to be explainable to some degree, and this is why linear regression comes into play.\n",
    "\n",
    "It really doesn't matter in which set you calculate this, as long as it is representative of the population you want to discuss. Here, we do it just for the train set.\n",
    "\n",
    "The first step is to get the predictions from the (main) random forest that we try to whitebox in the first place. Then, we will prepare a series of univariate (1D) regressions. To make the results comparable, we standardize all features. Note that the set of predictions (which is used as the outcome for the regressions) itself is also standardized, to make the beta's more interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relating feature importances to the target with linear surrogate models\n",
    "from sklearn.linear_model import LinearRegression  # We will be needing our linear regression class\n",
    "\n",
    "# Getting predicted values for the model we want to inspect\n",
    "rf_profit_best_preds_train = rf_profit_best.predict(X=X_train)\n",
    "\n",
    "# Standardize y\n",
    "y_pred_norm = np.array((rf_profit_best_preds_train-rf_profit_best_preds_train.mean()) / np.sqrt(rf_profit_best_preds_train.var())).reshape(-1, 1)\n",
    "\n",
    "# Fit a univariate linear regression for all predictors separately, using predicted value from Random Forest as target here!\n",
    "beta_surrogate_summary = pd.Series(index=X_train.columns, name='beta', dtype='float')  # Initialize empty Pandas Series with column names as rowindex\n",
    "for feat in X_train.columns:\n",
    "    current_X = np.array(X_train[feat]).reshape(-1, 1)  # because 1D regression\n",
    "    reg_surrogate = LinearRegression().fit(X=current_X, y=y_pred_norm)\n",
    "    \n",
    "    # Extracting estimated beta and putting it in our big 'beta' overview Series, using .at[]\n",
    "    beta_surrogate_summary.at[feat] = reg_surrogate.coef_[0] \n",
    "\n",
    "# Checking result on 5 coeffs\n",
    "print('5 coefficients of our surrogate univariate regressions for explainability, summarized:')\n",
    "beta_surrogate_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top k (20) univariate regression coefs in absolute value\n",
    "_k = 20\n",
    "# TRICK TO SORT ON ABSOLUTE VALUE (key=abs)!\n",
    "beta_surrogate_summary_top_k = (beta_surrogate_summary.sort_values(key=abs, ascending=False)\n",
    "                                .head(_k)\n",
    "                                .sort_values(key=abs, ascending=True)) # trick to order on absolute value :)\n",
    "\n",
    "# Plotting\n",
    "plt.barh(y=beta_surrogate_summary_top_k.index, width=beta_surrogate_summary_top_k, color='deepskyblue');\n",
    "plt.xlabel('Normalized univariate regression coefficient')\n",
    "plt.ylabel('Feature');\n",
    "plt.title(f\"Top {_k} regression coefficients to explain model rf_profit_bes\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing strongest top 5 negative features\n",
    "print('Most negative univariate betas:')\n",
    "beta_surrogate_summary.sort_values(ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing strongest top 5 positive features\n",
    "print('Most positive univariate betas:')\n",
    "beta_surrogate_summary.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the permuation importance features do on the univariate surrogate models\n",
    "beta_surrogate_summary.loc[rf_profit_best_df_top_k['feature']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate multiple linear regression model (only on top 20 models of permutation importance on rf1)\n",
    "# Getting normalized features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit_transform(X=X_train[rf_profit_best_df_top_k['feature']])\n",
    "\n",
    "X_train_norm = scaler.transform(X_train[rf_profit_best_df_top_k['feature']])\n",
    "y_pred_train_norm = np.array((rf_profit_best_preds_train - rf_profit_best_preds_train.mean()) / np.sqrt(rf_profit_best_preds_train.var())).reshape(-1, 1)\n",
    "\n",
    "# Fitting model\n",
    "multiple_surrogate_reg = LinearRegression().fit(X=X_train_norm, y=y_pred_train_norm)\n",
    "# Do .values otherwise df will be mangled up! due to non-sorted indices of rf1_fi_perm_df_top_k\n",
    "beta_surrogate_multiple_summary = pd.DataFrame(data={'feature': rf_profit_best_df_top_k['feature'].values,\n",
    "                                                     'coef': multiple_surrogate_reg.coef_[0]})\n",
    "print(beta_surrogate_multiple_summary.sort_values(by='coef', key=abs, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why is the prediction what it is\n",
    "### tree interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[44, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why is the prediction what it is?\n",
    "# Instance (observation)-based explanations using treeinterpreter package\n",
    "# Import tree interpreter functionalities\n",
    "from treeinterpreter import treeinterpreter as ti, utils\n",
    "\n",
    "# We choose a single row (instance, observation) to explain\n",
    "selected_ids = [44]  # hotel guest nr. 44\n",
    "selected_df = X_train.loc[selected_ids, :].values  #use .loc (because our 'Id' column is now the index :))\n",
    "\n",
    "# Getting predictions, biases and contributions\n",
    "prediction, bias, contributions = ti.predict(model=rf_profit_best, X=selected_df)\n",
    "\n",
    "for i in range(len(selected_ids)):\n",
    "    print('Guest Id', selected_ids[i])\n",
    "    print('Prediction:', prediction[i][0], 'Actual Value:', y_train[selected_ids[i]])\n",
    "    print('Bias (trainset mean)', bias[i])\n",
    "    print('Feature contributions:')\n",
    "    for c, feature in sorted(zip(contributions[i], X_train.columns), key=lambda x: -abs(x[0])):\n",
    "        # Only show nonzero - cleaner\n",
    "        if c != 0.0:\n",
    "            print(feature, round(c, 2))\n",
    "            print('-' * 20)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance (observation)-based explanations using LIME\n",
    "# Importing packages\n",
    "#import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Taking features that have less than 10 unique values as categorical, getting their INDEX! (np.argwhere())\n",
    "cat_feats_idx = np.argwhere(np.array([len(set(X_train.values[:, x])) \n",
    "                                      for x in range(X_train.values.shape[1])]) <= 10).flatten()\n",
    "# Instantiating LIME LimeTabularExplainer class\n",
    "explainer = LimeTabularExplainer(training_data=X_train.values,  # Needs to be np.array, hence .values, bit weird!\n",
    "                                 mode='regression',\n",
    "                                 categorical_features=cat_feats_idx,\n",
    "                                 feature_names=X_train.columns,\n",
    "                                 discretize_continuous=True,\n",
    "                                 random_state=seed)\n",
    "\n",
    "# Doing actual explanation using .explain_instance()\n",
    "explanation = explainer.explain_instance(data_row=X_train.loc[selected_ids[0]].values,   \n",
    "                                         predict_fn=rf_profit_best.predict, \n",
    "                                         num_features=10)\n",
    "explanation.show_in_notebook(show_all=False) # we ask lime to only display the features used in the explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing shap package\n",
    "import shap\n",
    "\n",
    "# Instantiate and explain using shap\n",
    "explainer = shap.TreeExplainer(model=rf_profit_best)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# 'bar'-type summary plot\n",
    "shap.summary_plot(shap_values=shap_values, \n",
    "                  features=X_train, \n",
    "                  plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Lift curve\n",
    "lift = cuf.plot_lift_curve(X_train,y_train, rf_damage_inc_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation with a force plot - GDB\n",
    "#shap.plots.force(shap_values[0])\n",
    "# explainer = shap.Explainer(model=rf_profit_best)\n",
    "# shap_values = explainer(X_train)\n",
    "# shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected list of hotel guest and what does the manager gain in applying the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score['revenue_pred'] = rf_profit_best.predict(score) - rf_damageAmount.predict(score)\n",
    "\n",
    "selection = score.sort_values('revenue_pred', ascending=False).head(200)\n",
    "sample = score.sample(200)\n",
    "\n",
    "total_revenue_selection = selection['revenue_pred'].sum()\n",
    "total_revenue_sample = sample['revenue_pred'].sum()\n",
    "\n",
    "print('The estimated revenue of the selection is %.3f, ' % total_revenue_selection)\n",
    "print('The estimated revenue of the sample is %.3f, ' % total_revenue_sample)\n",
    "print('This means that by applying the selection the model identified you gain %.3f!!! ' % (total_revenue_selection - total_revenue_sample))\n",
    "#print('Train R2: %.3f' % gbm.score(X=X_train, y=y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For EXECUTIVE SUMMARY : \n",
    "\n",
    "## Winst 200 geselecteerde tov winst voor random sample van hotel guest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis conducted using various algorithms like Random Forest, GBM, and SVR on the training data, a list of hotel guests has been selected for targeted marketing efforts. The best-performing model was identified through cross-validation and hyperparameter tuning.\n",
    "\n",
    "The gain achieved from the selected list of hotel guests is significant, with a calculated gain of 152,189.787 compared to the revenue from a random sample, which was 343,572.614. This indicates that targeting the selected list of hotel guests can potentially lead to increased revenue for the hotel.\n",
    "\n",
    "It is recommended that the hotel manager consider applying the selected list of hotel guests for tailored marketing strategies to capitalize on the potential revenue increase. By focusing efforts on these specific guests, the hotel can optimize its marketing initiatives and enhance overall profitability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
